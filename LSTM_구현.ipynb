{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYKGaiOUuMaSrH4xtRrVFK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zpKF1PlF6USF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMCell(nn.Module):\n",
        "  \"\"\"\n",
        "  seq_length : 332일치의 데이터가 있을 때 seq_length가 10이라면 10일치의 데이터가 하나의 시퀀스임. 그리고 시퀀스 개수는 332-10+1=323개.\n",
        "\n",
        "  batch_size : 여러 개의 시퀀스를 한 번에 처리하기 위한 묶음. 332일치의 데이터가 있을 때 batch_size가 32라면 10개의 시퀀스가 32개(연속된\n",
        "               320일 치의 데이터가 있다는 게 아님). 그러므로 이 경우 배치 하나의 shape은 (32,10,2).\n",
        "\n",
        "  hidden_size : hidden_state의 dimension(shape OR feature 개수). hidden_state의 dimension은 input_data의 dimension(feature)과 독립.\n",
        "                hidden_state는 과거의 모든 입력 정보와 LSTM의 내부 계산을 반영하여 다음 타임스텝으로 전달하는 역할이기 때문. 즉,\n",
        "                LSTM이 input_data를 처리한 결과로 생성된 학습 표현(learned representation).\n",
        "\n",
        "  hidden_state의 shape은 (num_layers, batch_size, hidden_size), input_data의 shape은 (batch_size, seq_length, feature_size).\n",
        "  cell_state의 shape은 (num_layers, batch_size, hidden_size), output의 shape은 (batch_size, seq_length, hidden_size). 이때 각 LSTM Cell에 들어가는\n",
        "  Xt의 shape은 (1,1,feature_size).\n",
        "  => 가중치 행렬은 이의 shape을 맞춰줘야 함.\n",
        "\n",
        "  nn.Module은 파이토치에서 모든 nueral network 구성에 필수. 미분과 매개변수 관리 등을 자동으로 해줌\n",
        "\n",
        "  nn.Module을 상속하게 되면 기본적으로 __init__()과 foward()를 override하게 됨\n",
        "\n",
        "  파라미터 reset 필요.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # 모델의 변수를 초기화하는 매서드\n",
        "  def __init__(self, input_size, seq_length, hidden_size, bias=True):\n",
        "    super(LSTMCell, self).__init__()\n",
        "    # 각 Cell에 들어올 input data feature(차원, dimension) 개수(크기)\n",
        "    self.input_size = input_size\n",
        "    # 각 Cell에 들어올 input data 개수\n",
        "    self.seq_length = seq_length\n",
        "    # hidden state의 feature(차원, dimension) 개수(크기)\n",
        "    self.hidden_size = hidden_size\n",
        "    # 편향\n",
        "    self.bias = bias\n",
        "\n",
        "    # 학습에 필요한 input gate 파라미터들을 생성\n",
        "    self.Wx_i = torch.Parameters(torch.Tensor((self.input_size, self.hidden_size))) # x에 대한 가중치 탠서\n",
        "    self.Wh_i = torch.Parameters(torch.Tensor((self.hidden_size, self.hidden_size))) # h에 대한 가중치 탠서\n",
        "    self.b_i = torch.Parameters(torch.Tensor((self.hidden_size, self.hidden_size)))\n",
        "    # 학습에 필요한 forget gate 파라미터들을 생성\n",
        "    self.Wx_f = torch.Parameters(torch.Tensor((self.input_size, self.hidden_size)))\n",
        "    self.Wh_f = torch.Parameters(torch.Tensor((self.hidden_size, self.hidden_size)))\n",
        "    self.b_f = torch.Parameters(torch.Tensor((self.hidden_size, self.hidden_size)))\n",
        "    # 학습에 필요한 input gate 파라미터들을 생성\n",
        "    self.Wx_o = torch.Parameters(torch.Tensor((self.input_size, self.hidden_size)))\n",
        "    self.Wh_o = torch.Parameters(torch.Tensor((self.hidden_size, self.hidden_size)))\n",
        "    self.b_o = torch.Parameters(torch.Tensor((self.hidden_size, self.hidden_size)))\n",
        "    # 학습에 필요한 예비 cell_state 파라미터들을 생성\n",
        "    self.Wx_c = torch.Parameters(torch.Tensor((self.input_size, self.hidden_size)))\n",
        "    self.Wh_c = torch.Parameters(torch.Tensor((self.hidden_size, self.hidden_size)))\n",
        "    self.b_c = torch.Parameters(torch.Tensor((self.hidden_size, self.hidden_size)))\n",
        "\n",
        "  # input data에 모델이 적용할 연산을 구현하는 매서드\n",
        "  def forward(self,x,hidden):\n",
        "    # hidden 변수에 이전 시점의 hidden_state, cell_state가 들어있음. set형태로.\n",
        "    h_prev, c_prev = hidden\n",
        "\n",
        "    # Input gate\n",
        "    i_t = torch.sigmoid(x @ self.Wx_i + h_prev @ self.Wh_i + self.b_i)\n",
        "\n",
        "    # Forget gate\n",
        "    f_t = torch.sigmoid(x @ self.Wx_f + h_prev @ self.Wh_f + self.b_f)\n",
        "\n",
        "    # 예비 cell_state값\n",
        "    c_tilde = torch.tanh(x @ self.Wx_c + h_prev @ self.Wh_c + self.b_c)\n",
        "\n",
        "    # Output gate\n",
        "    o_t = torch.sigmoid(x @ self.Wx_o + h_prev @ self.Wh_o + self.b_o)\n",
        "\n",
        "    # Cell state\n",
        "    c_t = f_t * c_prev + i_t * c_tilde\n",
        "\n",
        "    # hidden state\n",
        "    h_t = o_t * torch.tanh(c_t)\n",
        "\n",
        "    return h_t, c_t\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, num_layers):\n",
        "    self.num_layers = num_layers"
      ],
      "metadata": {
        "id": "PW5GdA436YjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 2\n",
        "seq_length = 10\n",
        "hidden_size = 100\n",
        "batch_size = 8\n",
        "x = torch.randn([1,1,input_size])\n",
        "h_t = torch.randn([1, batch_size, hidden_size])\n",
        "Wx_i = torch.randn([input_size, hidden_size])\n",
        "Wh_i = torch.randn([hidden_size, hidden_size])"
      ],
      "metadata": {
        "id": "ywSbRKGU_Ioy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sigmoid(x @ Wx_i + h_t @Wh_i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8W3Mv2MrWT28",
        "outputId": "2c760ab9-9f09-4ae0-bd69-6dbaf1884e0a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[9.9981e-01, 1.2498e-01, 2.8881e-02, 9.9973e-01, 9.9999e-01,\n",
              "          1.0000e+00, 3.7503e-04, 9.6060e-02, 8.7857e-04, 1.0000e+00,\n",
              "          1.1323e-03, 1.2335e-04, 4.8179e-04, 1.6290e-03, 9.9990e-01,\n",
              "          9.2669e-01, 9.9997e-01, 9.9990e-01, 6.3876e-09, 9.9363e-01,\n",
              "          1.2494e-01, 5.6763e-03, 1.0337e-03, 5.0390e-06, 8.7731e-01,\n",
              "          6.0464e-01, 3.2510e-04, 3.9635e-05, 9.8400e-01, 9.6908e-01,\n",
              "          6.8506e-04, 1.2726e-02, 2.8434e-04, 3.3788e-09, 1.6714e-11,\n",
              "          9.9987e-01, 2.8926e-08, 2.6368e-04, 9.8485e-01, 1.0000e+00,\n",
              "          9.9392e-05, 5.8612e-04, 1.0000e+00, 8.7897e-09, 9.9815e-01,\n",
              "          1.0000e+00, 9.9624e-01, 9.8852e-01, 9.9996e-01, 9.9995e-01,\n",
              "          9.9823e-01, 9.7229e-01, 9.9979e-01, 9.9947e-01, 2.9737e-05,\n",
              "          4.0152e-05, 4.9982e-01, 9.3108e-01, 1.0000e+00, 5.1607e-06,\n",
              "          3.1855e-03, 9.9794e-01, 2.4156e-05, 9.4130e-01, 9.9999e-01,\n",
              "          8.9068e-03, 9.0774e-01, 9.2075e-01, 3.9013e-03, 9.3578e-07,\n",
              "          2.5513e-04, 3.1859e-06, 9.8424e-01, 8.9111e-09, 4.6119e-02,\n",
              "          3.3175e-03, 6.3269e-03, 7.8352e-01, 9.7786e-01, 9.9794e-01,\n",
              "          1.0000e+00, 9.6820e-01, 9.9957e-01, 8.7988e-01, 9.9998e-01,\n",
              "          1.9918e-04, 9.4758e-01, 1.0910e-02, 9.9967e-01, 2.3249e-03,\n",
              "          9.9990e-01, 3.8907e-01, 2.8350e-06, 2.7337e-02, 1.0427e-01,\n",
              "          9.4144e-01, 1.2371e-03, 1.1367e-06, 3.8231e-01, 9.9983e-01],\n",
              "         [8.8540e-01, 1.0981e-04, 1.0000e+00, 8.0966e-01, 2.9196e-06,\n",
              "          9.7650e-01, 1.0000e+00, 1.2881e-04, 4.8217e-03, 9.9911e-01,\n",
              "          1.5628e-10, 9.5096e-01, 5.3870e-02, 5.1211e-05, 4.4246e-12,\n",
              "          3.2653e-04, 9.9999e-01, 9.9104e-01, 9.9997e-01, 1.7492e-03,\n",
              "          9.9512e-01, 3.5119e-02, 9.9546e-12, 1.8845e-03, 2.8472e-06,\n",
              "          2.4246e-02, 9.9981e-01, 1.0000e+00, 5.8453e-11, 1.6466e-06,\n",
              "          1.0000e+00, 1.6246e-02, 9.4248e-05, 6.2839e-03, 9.9662e-01,\n",
              "          7.3902e-02, 9.9727e-01, 3.3583e-01, 1.0000e+00, 9.9807e-01,\n",
              "          9.9592e-01, 4.7056e-01, 1.0000e+00, 2.9586e-01, 9.8818e-01,\n",
              "          1.0000e+00, 9.9340e-01, 4.5814e-01, 9.9766e-01, 6.9738e-01,\n",
              "          1.4796e-02, 2.9246e-01, 1.0000e+00, 9.6932e-01, 1.0000e+00,\n",
              "          9.4611e-02, 3.2067e-02, 3.3530e-08, 9.9998e-01, 1.2418e-03,\n",
              "          6.3766e-07, 1.2814e-03, 3.8789e-10, 3.9195e-06, 1.0247e-01,\n",
              "          9.9972e-01, 9.2311e-01, 9.6076e-06, 3.3493e-03, 9.8930e-01,\n",
              "          4.9226e-01, 1.5564e-03, 9.9995e-01, 9.9997e-01, 9.9510e-01,\n",
              "          8.8542e-02, 7.0474e-01, 1.0000e+00, 9.8885e-01, 2.8109e-01,\n",
              "          4.5436e-02, 9.7906e-01, 1.7958e-03, 1.0000e+00, 9.9147e-01,\n",
              "          4.0049e-04, 7.0689e-05, 3.8933e-01, 2.7025e-02, 4.5827e-01,\n",
              "          2.8137e-05, 4.1244e-01, 1.0181e-14, 9.9976e-01, 6.9685e-01,\n",
              "          1.0000e+00, 2.1834e-06, 1.4780e-06, 9.2612e-01, 9.8350e-01],\n",
              "         [7.5136e-01, 1.6041e-01, 1.0000e+00, 9.9999e-01, 9.7012e-01,\n",
              "          4.3606e-05, 9.1153e-01, 1.1866e-03, 1.0000e+00, 5.3396e-01,\n",
              "          3.9203e-05, 3.3901e-06, 6.4698e-01, 8.9992e-01, 9.9984e-01,\n",
              "          3.7432e-12, 5.4893e-04, 9.1544e-01, 9.0609e-01, 2.2421e-02,\n",
              "          4.4787e-01, 1.6398e-06, 7.5814e-01, 5.5098e-04, 2.4852e-05,\n",
              "          1.3078e-01, 9.9988e-01, 3.0747e-02, 1.0000e+00, 9.4047e-01,\n",
              "          6.6284e-01, 1.0932e-04, 2.2838e-04, 9.9991e-01, 2.6841e-03,\n",
              "          9.9980e-01, 2.9125e-03, 7.6854e-01, 6.5408e-04, 7.9548e-02,\n",
              "          2.1904e-05, 2.9605e-12, 1.0000e+00, 3.0668e-03, 2.4275e-02,\n",
              "          3.5097e-01, 2.5741e-02, 9.9999e-01, 8.3675e-02, 9.7279e-01,\n",
              "          9.9983e-01, 1.1190e-04, 6.5922e-01, 9.4375e-02, 3.4704e-06,\n",
              "          5.1044e-01, 3.6765e-01, 1.2181e-03, 2.1823e-03, 9.0273e-02,\n",
              "          4.1517e-05, 9.9985e-01, 9.6058e-04, 4.7976e-01, 9.0364e-10,\n",
              "          3.9178e-06, 8.0999e-01, 7.9627e-02, 9.9966e-01, 9.7554e-01,\n",
              "          9.2692e-01, 8.0925e-04, 2.7603e-02, 9.9997e-01, 9.5570e-10,\n",
              "          2.0064e-03, 2.4717e-07, 3.1340e-10, 8.4969e-01, 1.8762e-04,\n",
              "          1.7363e-02, 2.7805e-05, 1.0000e+00, 1.0000e+00, 9.9866e-01,\n",
              "          1.0000e+00, 9.9994e-01, 6.4317e-05, 1.0000e+00, 2.6506e-05,\n",
              "          9.9287e-01, 9.9945e-01, 7.6061e-04, 9.9991e-01, 1.5464e-01,\n",
              "          2.2797e-01, 1.2465e-03, 2.8570e-02, 9.9977e-01, 1.0000e+00],\n",
              "         [7.4192e-01, 9.9996e-01, 1.0000e+00, 4.5982e-06, 9.9861e-01,\n",
              "          1.3376e-05, 9.9476e-01, 8.8174e-01, 1.1250e-02, 1.0000e+00,\n",
              "          7.7560e-04, 1.8096e-05, 2.3698e-01, 9.9998e-01, 4.6043e-05,\n",
              "          3.6530e-02, 1.0000e+00, 1.8423e-01, 6.1126e-04, 8.7450e-01,\n",
              "          3.8654e-04, 5.1998e-02, 1.2893e-03, 2.5606e-04, 1.5739e-05,\n",
              "          9.7053e-01, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
              "          7.1207e-01, 1.4267e-02, 2.6907e-07, 6.0973e-03, 1.9433e-02,\n",
              "          3.0905e-04, 9.9999e-01, 9.9997e-01, 9.9991e-01, 3.0837e-04,\n",
              "          9.9922e-01, 9.9933e-01, 3.3518e-12, 9.7213e-01, 2.9437e-04,\n",
              "          1.0000e+00, 7.0406e-01, 1.3216e-03, 7.5897e-01, 9.9688e-01,\n",
              "          7.5519e-01, 9.9991e-01, 9.8374e-01, 1.5600e-03, 1.0000e+00,\n",
              "          2.6419e-04, 5.4222e-02, 9.9999e-01, 9.9993e-01, 8.9706e-01,\n",
              "          8.4830e-01, 2.3068e-07, 5.2035e-05, 1.0000e+00, 1.4337e-01,\n",
              "          5.3671e-09, 8.8058e-01, 2.0572e-05, 9.9717e-01, 1.0000e+00,\n",
              "          3.1297e-03, 9.4447e-01, 7.4364e-05, 9.9991e-01, 9.6579e-01,\n",
              "          9.2017e-01, 9.9994e-01, 9.9952e-01, 9.9693e-01, 1.5306e-04,\n",
              "          7.8890e-01, 2.6164e-05, 2.3979e-04, 1.5147e-03, 9.9996e-01,\n",
              "          6.3247e-03, 1.0000e+00, 9.7748e-01, 8.6450e-03, 1.2223e-07,\n",
              "          9.9976e-01, 1.0000e+00, 9.9948e-01, 1.0000e+00, 5.5747e-03,\n",
              "          1.0000e+00, 1.1088e-02, 2.1621e-09, 1.0000e+00, 9.9900e-01],\n",
              "         [1.1751e-04, 1.9332e-06, 8.5173e-01, 6.5731e-09, 5.1518e-01,\n",
              "          9.9990e-01, 2.3179e-02, 2.2759e-05, 1.0167e-04, 2.3082e-01,\n",
              "          3.2479e-02, 1.8620e-05, 7.5929e-01, 2.2735e-06, 2.6345e-01,\n",
              "          6.8011e-08, 3.1856e-02, 6.0056e-10, 2.7612e-03, 9.9994e-01,\n",
              "          4.5448e-01, 2.2086e-02, 3.0214e-11, 9.9932e-01, 5.1617e-01,\n",
              "          1.5321e-01, 1.0000e+00, 1.0000e+00, 6.0320e-01, 3.9293e-04,\n",
              "          1.0000e+00, 1.0000e+00, 7.6658e-05, 2.0981e-03, 1.0311e-07,\n",
              "          1.0000e+00, 1.7434e-01, 1.2419e-01, 3.9286e-06, 1.5870e-02,\n",
              "          9.9997e-01, 9.3798e-01, 8.5926e-01, 1.1060e-04, 6.1398e-03,\n",
              "          4.2417e-01, 1.0000e+00, 1.8320e-02, 5.7587e-05, 9.9984e-01,\n",
              "          1.6600e-04, 9.9998e-01, 5.1650e-01, 4.9866e-02, 7.8090e-01,\n",
              "          4.9335e-06, 9.9997e-01, 9.8660e-01, 1.0260e-03, 9.9969e-01,\n",
              "          1.0000e+00, 1.8289e-02, 2.0353e-06, 9.9988e-01, 2.0560e-09,\n",
              "          9.5247e-01, 8.7835e-05, 2.2951e-03, 5.5381e-07, 2.2590e-05,\n",
              "          3.5280e-04, 9.9279e-01, 9.9926e-01, 2.6827e-03, 1.0000e+00,\n",
              "          1.0000e+00, 6.0263e-01, 1.0000e+00, 1.9418e-01, 5.3189e-03,\n",
              "          9.2564e-01, 9.9990e-01, 7.6549e-04, 8.4827e-03, 3.1144e-02,\n",
              "          1.0195e-05, 9.9113e-01, 2.5184e-03, 9.9999e-01, 1.8751e-01,\n",
              "          1.5960e-01, 2.0367e-07, 4.9418e-02, 2.3555e-01, 9.1809e-07,\n",
              "          1.4111e-06, 1.0000e+00, 7.6245e-05, 4.1777e-02, 2.9413e-04],\n",
              "         [2.6973e-03, 1.7223e-01, 1.2558e-04, 1.7046e-03, 2.0837e-03,\n",
              "          2.6403e-05, 5.4348e-05, 9.9593e-01, 9.3524e-04, 2.2092e-04,\n",
              "          3.5716e-04, 9.9995e-01, 9.5552e-04, 1.4213e-03, 9.9988e-01,\n",
              "          8.5292e-08, 2.0953e-02, 9.9961e-01, 9.8655e-01, 9.9990e-01,\n",
              "          9.9777e-01, 5.9875e-01, 1.0483e-05, 1.9800e-01, 9.4315e-01,\n",
              "          6.6777e-01, 8.7598e-01, 1.2971e-01, 9.9120e-01, 9.9622e-01,\n",
              "          9.9992e-01, 3.6163e-07, 6.2593e-04, 9.9721e-01, 4.8492e-06,\n",
              "          1.0000e+00, 9.6613e-01, 9.7346e-01, 9.9998e-01, 2.6655e-09,\n",
              "          3.1921e-12, 5.3410e-08, 9.1467e-01, 1.6719e-04, 9.9933e-01,\n",
              "          2.9672e-01, 3.1602e-06, 9.7943e-01, 2.3848e-02, 5.2355e-01,\n",
              "          9.2296e-01, 9.5462e-01, 9.9995e-01, 3.5859e-03, 1.8209e-07,\n",
              "          8.3371e-01, 1.8321e-03, 4.9118e-07, 9.8159e-01, 8.9742e-01,\n",
              "          9.9806e-01, 1.0000e+00, 9.9998e-01, 9.9992e-01, 1.6451e-07,\n",
              "          9.0369e-01, 5.8986e-02, 9.9899e-01, 9.8074e-01, 7.5910e-04,\n",
              "          9.4631e-01, 1.2969e-02, 8.9446e-01, 2.8075e-04, 2.3542e-02,\n",
              "          5.3716e-02, 2.6184e-09, 1.9634e-01, 2.5660e-02, 1.2960e-04,\n",
              "          9.9992e-01, 6.7463e-02, 2.0416e-02, 4.8111e-04, 1.9488e-03,\n",
              "          1.2305e-01, 7.8076e-01, 1.1654e-02, 9.9854e-01, 2.0123e-03,\n",
              "          1.4027e-04, 9.9370e-04, 9.9786e-01, 7.9713e-01, 9.9957e-01,\n",
              "          9.9832e-01, 2.8370e-01, 9.8944e-01, 5.9021e-13, 1.1179e-07],\n",
              "         [2.1471e-01, 3.2294e-02, 1.0000e+00, 2.0113e-03, 9.9933e-01,\n",
              "          7.0717e-06, 1.8619e-04, 9.9884e-01, 6.4721e-01, 9.9910e-01,\n",
              "          8.7337e-01, 5.4088e-02, 9.9949e-01, 1.0000e+00, 9.9086e-01,\n",
              "          6.6680e-05, 5.6073e-07, 7.6138e-02, 9.9912e-01, 9.6371e-01,\n",
              "          9.9998e-01, 2.6732e-06, 8.3359e-07, 1.2193e-04, 9.9983e-01,\n",
              "          2.2839e-05, 1.1632e-02, 4.2761e-02, 1.0000e+00, 1.0000e+00,\n",
              "          1.9745e-04, 9.9995e-01, 5.5989e-02, 1.0000e+00, 1.0000e+00,\n",
              "          9.6951e-01, 6.6217e-12, 2.1808e-04, 4.8930e-01, 9.9861e-01,\n",
              "          6.4375e-01, 2.3167e-01, 9.3456e-01, 4.7917e-08, 9.2420e-01,\n",
              "          3.1308e-02, 9.3925e-01, 9.8108e-01, 2.2515e-02, 2.1071e-04,\n",
              "          9.9999e-01, 7.8607e-05, 7.4919e-07, 9.1417e-01, 3.3641e-03,\n",
              "          9.0708e-02, 9.7744e-01, 2.6210e-06, 1.7798e-11, 3.7935e-02,\n",
              "          1.0000e+00, 1.0000e+00, 1.0000e+00, 9.6996e-01, 9.9922e-01,\n",
              "          1.0000e+00, 8.7062e-07, 9.8511e-01, 9.9775e-01, 1.1381e-01,\n",
              "          1.2742e-05, 9.9913e-01, 1.4971e-05, 9.9977e-01, 8.8812e-01,\n",
              "          1.0000e+00, 1.8005e-12, 5.8138e-08, 1.0000e+00, 7.5201e-06,\n",
              "          9.9643e-01, 4.2090e-01, 9.9940e-01, 6.2846e-01, 4.0655e-07,\n",
              "          8.2168e-01, 9.9159e-01, 9.9311e-01, 9.7667e-01, 1.2834e-07,\n",
              "          1.0000e+00, 9.5862e-01, 7.6131e-04, 9.0907e-01, 7.1693e-03,\n",
              "          4.8017e-06, 9.9841e-01, 1.0000e+00, 2.1533e-05, 1.4804e-01],\n",
              "         [1.0000e+00, 7.7942e-01, 9.9964e-01, 9.8598e-01, 3.5076e-02,\n",
              "          9.9998e-01, 9.9935e-01, 1.0000e+00, 1.9808e-02, 6.7639e-05,\n",
              "          7.1555e-07, 3.9460e-01, 9.1591e-01, 9.8789e-01, 1.2349e-03,\n",
              "          1.4566e-01, 9.9823e-01, 4.9013e-02, 8.2061e-02, 9.9877e-01,\n",
              "          1.0000e+00, 7.5854e-01, 7.7354e-01, 2.0155e-02, 3.1404e-08,\n",
              "          2.8332e-01, 9.9790e-01, 9.9997e-01, 9.6080e-01, 1.1632e-06,\n",
              "          1.0000e+00, 4.2631e-01, 1.0000e+00, 7.3952e-02, 1.9070e-04,\n",
              "          4.7382e-02, 5.1555e-01, 9.9986e-01, 9.5541e-01, 1.8395e-05,\n",
              "          9.8460e-02, 9.0736e-01, 1.6678e-02, 1.7982e-04, 9.1942e-01,\n",
              "          9.9744e-01, 3.5882e-08, 9.7286e-01, 5.4175e-06, 1.8253e-04,\n",
              "          1.9442e-05, 8.6686e-01, 8.9122e-02, 9.2111e-01, 5.8512e-08,\n",
              "          5.5187e-04, 6.0740e-04, 1.7420e-04, 1.4886e-05, 2.3720e-04,\n",
              "          8.5992e-02, 9.9998e-01, 1.2464e-01, 3.9296e-05, 1.0000e+00,\n",
              "          9.4192e-08, 8.5244e-03, 9.9973e-01, 1.7739e-06, 6.4007e-02,\n",
              "          8.8856e-01, 6.4982e-02, 9.6849e-01, 9.8679e-01, 3.8546e-02,\n",
              "          1.3032e-04, 9.9968e-01, 2.8747e-02, 9.5231e-01, 1.7126e-05,\n",
              "          9.9995e-01, 7.8324e-09, 1.8501e-05, 1.2280e-04, 7.9489e-01,\n",
              "          9.9961e-01, 9.9965e-01, 1.5825e-02, 1.7604e-02, 7.2716e-01,\n",
              "          5.9040e-06, 9.9989e-01, 3.4224e-11, 1.0000e+00, 8.7887e-04,\n",
              "          9.9996e-01, 2.6598e-02, 9.6315e-01, 8.8723e-01, 9.6255e-06]]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터는 332X2임. feature는 가격과 날짜. 총 332일치의 나스닥지수 데이터"
      ],
      "metadata": {
        "id": "06hYmUrL_Psk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}