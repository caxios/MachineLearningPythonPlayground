{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsX9gIvdCw4SOTG4VEXNzL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#MNIST 예시"
      ],
      "metadata": {
        "id": "EoGe7_Jg6mfD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WgJu9Eq96l03"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# 신경망 구축에 필요한 여러 매서드를 담은 torch.nn\n",
        "import torch.nn as nn\n",
        "# torch.nn의 모든 함수를 포함, 손실/활성화/풀링/합성곱/선형 및 기타 신경망 함수 가 포함됨\n",
        "import torch.nn.functional as F\n",
        "# 최적화(신경망의 가중치,매개변수 조정을 위해 오차를 역전파 하는 과정) 모듈이 담김\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "  \"\"\"\n",
        "  kernel_size : 보통 홀수. 너무 작으면 픽셀을 처리하는 kernel이 이웃 픽셀의 정보를 가지지 못한다.너무 크면 이미지 내에서 정밀하지\n",
        "                않은 특징을 얻게 됨. 작은 kernel_size의 많은 layer를 쓰면 네트워크가 깊어지고, 더 복잡한 특징 학습 가능.\n",
        "\n",
        "  feature map : 이미지 데이터에서 픽셀 정보를 담고 있는 channel(차원)을 의미. 이미지에서 더 많은 특징을 추출하려거든 channel을 크게\n",
        "                하면 된다.\n",
        "\n",
        "  input shape이 28x28x1임\n",
        "\n",
        "  Conv2d(input_channel, output_channel, kernel_size, stride)\n",
        "  \"\"\"\n",
        "  # 각 layer의 뉴런개수 및 layer들 정의\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "\n",
        "    # 합성곱 layer\n",
        "    self.cn1 = nn.Conv2d(1,16,3,1)\n",
        "    self.cn2 = nn.Conv2d(16,32,3,1)\n",
        "    # 드롭아웃 layer\n",
        "    self.dp1 = nn.Dropout(0.1)\n",
        "    self.dp2 = nn.Dropout(0.25)\n",
        "    # fully-connected(fc) layer, 4608=12x12x32\n",
        "    self.fc1 = nn.Linear(4608, 64)\n",
        "    # 최종 출력은 10개 클래스 중 하나\n",
        "    self.fc2 = nn.Linear(64,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.cn1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.cn2(x)\n",
        "    x = F.relu(x)\n",
        "    # kernel size가 2x2\n",
        "    x = F.max_pool2d(x,2)\n",
        "    x = self.dp1(x)\n",
        "    # 1차원 백터로 평면화\n",
        "    x = torch.flatten(x,1)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dp2(x)\n",
        "    x = self.fc2(x)\n",
        "    # 모델의 예측을 out에 담음\n",
        "    out = F.log_softmax(x, dim=1)\n",
        "    return out"
      ],
      "metadata": {
        "id": "DYydgv6t6zdy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 루틴 정의\n",
        "def train(model, device, train_dataloader, optim, epoch):\n",
        "  # 모델 훈련\n",
        "  model.train()\n",
        "  # batch 단위로 반복\n",
        "  for b_i, (X,y) in enumerate(train_dataloader):\n",
        "    # 주어진 로컬 메모리에 데이터셋 사본 만듦\n",
        "    X,y = X.to(device), y.to(device)\n",
        "    # 이전에 계산했던 gradient를 초기화(이전 값들은 이미 이전단계의 파라미터 수정할 때 쓰였으니까 노필요)\n",
        "    optim.zero_grad()\n",
        "    # 주어진 입력데이터를 활용하여 모델 예측 실행\n",
        "    pred = model(X)\n",
        "    # negative log liklihood, 모델예측값과 실제값 사이의 손실 계산\n",
        "    loss = F.nll_loss(pred, y)\n",
        "    # 역전파, 자동미분됨\n",
        "    loss.backward()\n",
        "    # 가중치 조정\n",
        "    optim.step()\n",
        "    if b_i % 10 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, b_i * len(train_dataloader), len(train_dataloader.dataset),100. * b_i / len(train_dataloader), loss.item()))"
      ],
      "metadata": {
        "id": "1eWbfHJuOwBo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 루틴 정의\n",
        "def test(model, device, test_dataloader):\n",
        "  \"\"\"\n",
        "  torch.no_grad : 추론(평가) 과정에서 사용하는 autograd 끄는 함수. 모델이 파라미터 업데이트 안 하고(즉, 학습X) 단순히 예측/추론만 함.\n",
        "\n",
        "  평균적인 손실(오차) 정도를 구하기 위해 loss를 합한다.\n",
        "  \"\"\"\n",
        "\n",
        "  # 모델 성능 평가\n",
        "  model.eval()\n",
        "  loss = 0\n",
        "  success = 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataloader:\n",
        "      X,y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      # 배치별 손실의 합(옵티마이저X->모델 가중치 조정X->모델평가를 위해 배치 단위로 오차 합함)\n",
        "      loss += F.nll_loss(pred, y, reduction='sum').item()\n",
        "      pred = pred.argmax(dim=1, keepdim=True)\n",
        "      success += pred.eq(y.view_as(pred)).sum().item()\n",
        "    loss /= len(test_dataloader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(loss, success, len(test_dataloader.dataset),100. * success / len(test_dataloader.dataset)))"
      ],
      "metadata": {
        "id": "BYHX8xVWStHe"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "torch.DataLoader : DataLoader에 입력되는 dataset을 모델에 배치입력하기 쉽게 만들어주는 모듈(추가적인 동작이 필요할 경우 DataSet을 상속받는\n",
        "                   사용자 정의 DataSet을 만들어 그 DataSet을 DataLoader에 넣기도 함). 그래서 Dataset갹채에서 배치단위 데이터를 가져온 걸 반환.\n",
        "\n",
        "transform 파리미터는 MNIST 이미지 데이터에 대한 전처리 과정을 정의.\n",
        "\n",
        "transforms.Compose() 괄호 안에 들어가는 변환함수들을 담은 리스트는 순차적으로 입력데이터에 적용됨\n",
        "\n",
        "transforms.Normalize((mean, std))\n",
        "\n",
        "batch_size : 한번에 모델에 입력되는 데이터 묶음. 총 1000개의 이미지데이터가 있다면 batch_size가 32이라면 한 번 학습 시에\n",
        "             32개의 이미지(데이터,혹은 샘플)가 쓰인다는 것. 그리고 이때 1000/32 약 32개의 배치(데이터묶음)가 생기는데\n",
        "             마지막 배치의 경우 (1000-31)*32로 8개의 이미지데이터가 담김\n",
        "# # 훈련 데이터 불러오기\n",
        "# train_dataloader = torch.utils.data.DataLoader(datasets.MNIST(\"../data\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor(),\n",
        "#                                                                                                                                   transforms.ToTensor(),\n",
        "#                                                                                                                                   transforms.Normalize((0.1302,),(0.3069))])),\n",
        "#                                                batch_size=32, shuffle=True)\n",
        "# # 성능 평가를 위한 데이터 불러오기\n",
        "# test_dataloader = torch.utils.data.DataLoader(datasets.MNIST(\"../data\", train=False, transform=transforms.Compose([transforms.ToTensor(),\n",
        "#                                                                                                                    transforms.Normalize((0.1302,),(0.3069))])),\n",
        "#                                                batch_size=500, shuffle=True)\n",
        "\"\"\"\n",
        "\n",
        "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                    transform=transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset1,batch_size=32, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset2, batch_size=500, shuffle=True)"
      ],
      "metadata": {
        "id": "wW51u2_zVRcO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 무작위성X/재현가능성 위해 시드값 설정\n",
        "torch.manual_seed(0)\n",
        "# 연산 수행할 장치 지정\n",
        "device = torch.device(\"cpu\")\n",
        "# 모델 객체 생성\n",
        "model = ConvNet()\n",
        "# 옵티마이저 객체 생성\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
      ],
      "metadata": {
        "id": "da_7-URMYG_2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 실제로 훈련,테스트\n",
        "# %capture\n",
        "for epoch in range(1,3):\n",
        "  train(model, device, train_dataloader, optimizer, epoch)\n",
        "  test(model, device, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InMw35Y0Yic3",
        "outputId": "40487d1d-cd3a-46ca-e720-331dc20f2303"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.362310\n",
            "Train Epoch: 1 [18750/60000 (1%)]\tLoss: 0.093845\n",
            "Train Epoch: 1 [37500/60000 (1%)]\tLoss: 0.220424\n",
            "Train Epoch: 1 [56250/60000 (2%)]\tLoss: 0.221767\n",
            "Train Epoch: 1 [75000/60000 (2%)]\tLoss: 0.007746\n",
            "Train Epoch: 1 [93750/60000 (3%)]\tLoss: 0.042532\n",
            "Train Epoch: 1 [112500/60000 (3%)]\tLoss: 0.301133\n",
            "Train Epoch: 1 [131250/60000 (4%)]\tLoss: 0.080663\n",
            "Train Epoch: 1 [150000/60000 (4%)]\tLoss: 0.179335\n",
            "Train Epoch: 1 [168750/60000 (5%)]\tLoss: 0.082491\n",
            "Train Epoch: 1 [187500/60000 (5%)]\tLoss: 0.104114\n",
            "Train Epoch: 1 [206250/60000 (6%)]\tLoss: 0.186040\n",
            "Train Epoch: 1 [225000/60000 (6%)]\tLoss: 0.058609\n",
            "Train Epoch: 1 [243750/60000 (7%)]\tLoss: 0.076216\n",
            "Train Epoch: 1 [262500/60000 (7%)]\tLoss: 0.015135\n",
            "Train Epoch: 1 [281250/60000 (8%)]\tLoss: 0.077796\n",
            "Train Epoch: 1 [300000/60000 (9%)]\tLoss: 0.016076\n",
            "Train Epoch: 1 [318750/60000 (9%)]\tLoss: 0.004198\n",
            "Train Epoch: 1 [337500/60000 (10%)]\tLoss: 0.002653\n",
            "Train Epoch: 1 [356250/60000 (10%)]\tLoss: 0.048009\n",
            "Train Epoch: 1 [375000/60000 (11%)]\tLoss: 0.082033\n",
            "Train Epoch: 1 [393750/60000 (11%)]\tLoss: 0.220552\n",
            "Train Epoch: 1 [412500/60000 (12%)]\tLoss: 0.010468\n",
            "Train Epoch: 1 [431250/60000 (12%)]\tLoss: 0.022024\n",
            "Train Epoch: 1 [450000/60000 (13%)]\tLoss: 0.001819\n",
            "Train Epoch: 1 [468750/60000 (13%)]\tLoss: 0.027283\n",
            "Train Epoch: 1 [487500/60000 (14%)]\tLoss: 0.003357\n",
            "Train Epoch: 1 [506250/60000 (14%)]\tLoss: 0.136345\n",
            "Train Epoch: 1 [525000/60000 (15%)]\tLoss: 0.131424\n",
            "Train Epoch: 1 [543750/60000 (15%)]\tLoss: 0.010848\n",
            "Train Epoch: 1 [562500/60000 (16%)]\tLoss: 0.023066\n",
            "Train Epoch: 1 [581250/60000 (17%)]\tLoss: 0.037228\n",
            "Train Epoch: 1 [600000/60000 (17%)]\tLoss: 0.059686\n",
            "Train Epoch: 1 [618750/60000 (18%)]\tLoss: 0.022942\n",
            "Train Epoch: 1 [637500/60000 (18%)]\tLoss: 0.020151\n",
            "Train Epoch: 1 [656250/60000 (19%)]\tLoss: 0.001609\n",
            "Train Epoch: 1 [675000/60000 (19%)]\tLoss: 0.013472\n",
            "Train Epoch: 1 [693750/60000 (20%)]\tLoss: 0.207148\n",
            "Train Epoch: 1 [712500/60000 (20%)]\tLoss: 0.009627\n",
            "Train Epoch: 1 [731250/60000 (21%)]\tLoss: 0.042678\n",
            "Train Epoch: 1 [750000/60000 (21%)]\tLoss: 0.032171\n",
            "Train Epoch: 1 [768750/60000 (22%)]\tLoss: 0.119025\n",
            "Train Epoch: 1 [787500/60000 (22%)]\tLoss: 0.007277\n",
            "Train Epoch: 1 [806250/60000 (23%)]\tLoss: 0.084978\n",
            "Train Epoch: 1 [825000/60000 (23%)]\tLoss: 0.030454\n",
            "Train Epoch: 1 [843750/60000 (24%)]\tLoss: 0.025594\n",
            "Train Epoch: 1 [862500/60000 (25%)]\tLoss: 0.022793\n",
            "Train Epoch: 1 [881250/60000 (25%)]\tLoss: 0.074968\n",
            "Train Epoch: 1 [900000/60000 (26%)]\tLoss: 0.012787\n",
            "Train Epoch: 1 [918750/60000 (26%)]\tLoss: 0.037628\n",
            "Train Epoch: 1 [937500/60000 (27%)]\tLoss: 0.004986\n",
            "Train Epoch: 1 [956250/60000 (27%)]\tLoss: 0.087846\n",
            "Train Epoch: 1 [975000/60000 (28%)]\tLoss: 0.012010\n",
            "Train Epoch: 1 [993750/60000 (28%)]\tLoss: 0.040186\n",
            "Train Epoch: 1 [1012500/60000 (29%)]\tLoss: 0.064841\n",
            "Train Epoch: 1 [1031250/60000 (29%)]\tLoss: 0.028441\n",
            "Train Epoch: 1 [1050000/60000 (30%)]\tLoss: 0.250580\n",
            "Train Epoch: 1 [1068750/60000 (30%)]\tLoss: 0.209879\n",
            "Train Epoch: 1 [1087500/60000 (31%)]\tLoss: 0.003060\n",
            "Train Epoch: 1 [1106250/60000 (31%)]\tLoss: 0.047994\n",
            "Train Epoch: 1 [1125000/60000 (32%)]\tLoss: 0.029986\n",
            "Train Epoch: 1 [1143750/60000 (33%)]\tLoss: 0.009222\n",
            "Train Epoch: 1 [1162500/60000 (33%)]\tLoss: 0.101069\n",
            "Train Epoch: 1 [1181250/60000 (34%)]\tLoss: 0.019545\n",
            "Train Epoch: 1 [1200000/60000 (34%)]\tLoss: 0.101312\n",
            "Train Epoch: 1 [1218750/60000 (35%)]\tLoss: 0.012995\n",
            "Train Epoch: 1 [1237500/60000 (35%)]\tLoss: 0.254296\n",
            "Train Epoch: 1 [1256250/60000 (36%)]\tLoss: 0.052898\n",
            "Train Epoch: 1 [1275000/60000 (36%)]\tLoss: 0.007165\n",
            "Train Epoch: 1 [1293750/60000 (37%)]\tLoss: 0.042642\n",
            "Train Epoch: 1 [1312500/60000 (37%)]\tLoss: 0.003044\n",
            "Train Epoch: 1 [1331250/60000 (38%)]\tLoss: 0.056258\n",
            "Train Epoch: 1 [1350000/60000 (38%)]\tLoss: 0.114299\n",
            "Train Epoch: 1 [1368750/60000 (39%)]\tLoss: 0.008922\n",
            "Train Epoch: 1 [1387500/60000 (39%)]\tLoss: 0.042325\n",
            "Train Epoch: 1 [1406250/60000 (40%)]\tLoss: 0.041918\n",
            "Train Epoch: 1 [1425000/60000 (41%)]\tLoss: 0.251930\n",
            "Train Epoch: 1 [1443750/60000 (41%)]\tLoss: 0.015471\n",
            "Train Epoch: 1 [1462500/60000 (42%)]\tLoss: 0.315905\n",
            "Train Epoch: 1 [1481250/60000 (42%)]\tLoss: 0.007968\n",
            "Train Epoch: 1 [1500000/60000 (43%)]\tLoss: 0.022968\n",
            "Train Epoch: 1 [1518750/60000 (43%)]\tLoss: 0.084429\n",
            "Train Epoch: 1 [1537500/60000 (44%)]\tLoss: 0.295621\n",
            "Train Epoch: 1 [1556250/60000 (44%)]\tLoss: 0.007330\n",
            "Train Epoch: 1 [1575000/60000 (45%)]\tLoss: 0.036364\n",
            "Train Epoch: 1 [1593750/60000 (45%)]\tLoss: 0.068333\n",
            "Train Epoch: 1 [1612500/60000 (46%)]\tLoss: 0.260755\n",
            "Train Epoch: 1 [1631250/60000 (46%)]\tLoss: 0.039729\n",
            "Train Epoch: 1 [1650000/60000 (47%)]\tLoss: 0.087155\n",
            "Train Epoch: 1 [1668750/60000 (47%)]\tLoss: 0.007203\n",
            "Train Epoch: 1 [1687500/60000 (48%)]\tLoss: 0.116501\n",
            "Train Epoch: 1 [1706250/60000 (49%)]\tLoss: 0.009226\n",
            "Train Epoch: 1 [1725000/60000 (49%)]\tLoss: 0.009554\n",
            "Train Epoch: 1 [1743750/60000 (50%)]\tLoss: 0.055002\n",
            "Train Epoch: 1 [1762500/60000 (50%)]\tLoss: 0.199301\n",
            "Train Epoch: 1 [1781250/60000 (51%)]\tLoss: 0.182979\n",
            "Train Epoch: 1 [1800000/60000 (51%)]\tLoss: 0.009609\n",
            "Train Epoch: 1 [1818750/60000 (52%)]\tLoss: 0.012990\n",
            "Train Epoch: 1 [1837500/60000 (52%)]\tLoss: 0.022940\n",
            "Train Epoch: 1 [1856250/60000 (53%)]\tLoss: 0.091127\n",
            "Train Epoch: 1 [1875000/60000 (53%)]\tLoss: 0.071588\n",
            "Train Epoch: 1 [1893750/60000 (54%)]\tLoss: 0.056065\n",
            "Train Epoch: 1 [1912500/60000 (54%)]\tLoss: 0.247231\n",
            "Train Epoch: 1 [1931250/60000 (55%)]\tLoss: 0.089179\n",
            "Train Epoch: 1 [1950000/60000 (55%)]\tLoss: 0.030312\n",
            "Train Epoch: 1 [1968750/60000 (56%)]\tLoss: 0.036014\n",
            "Train Epoch: 1 [1987500/60000 (57%)]\tLoss: 0.010892\n",
            "Train Epoch: 1 [2006250/60000 (57%)]\tLoss: 0.032893\n",
            "Train Epoch: 1 [2025000/60000 (58%)]\tLoss: 0.076221\n",
            "Train Epoch: 1 [2043750/60000 (58%)]\tLoss: 0.016062\n",
            "Train Epoch: 1 [2062500/60000 (59%)]\tLoss: 0.006844\n",
            "Train Epoch: 1 [2081250/60000 (59%)]\tLoss: 0.004709\n",
            "Train Epoch: 1 [2100000/60000 (60%)]\tLoss: 0.015210\n",
            "Train Epoch: 1 [2118750/60000 (60%)]\tLoss: 0.004981\n",
            "Train Epoch: 1 [2137500/60000 (61%)]\tLoss: 0.021875\n",
            "Train Epoch: 1 [2156250/60000 (61%)]\tLoss: 0.034441\n",
            "Train Epoch: 1 [2175000/60000 (62%)]\tLoss: 0.313134\n",
            "Train Epoch: 1 [2193750/60000 (62%)]\tLoss: 0.001055\n",
            "Train Epoch: 1 [2212500/60000 (63%)]\tLoss: 0.054706\n",
            "Train Epoch: 1 [2231250/60000 (63%)]\tLoss: 0.004831\n",
            "Train Epoch: 1 [2250000/60000 (64%)]\tLoss: 0.067290\n",
            "Train Epoch: 1 [2268750/60000 (65%)]\tLoss: 0.042329\n",
            "Train Epoch: 1 [2287500/60000 (65%)]\tLoss: 0.003517\n",
            "Train Epoch: 1 [2306250/60000 (66%)]\tLoss: 0.136973\n",
            "Train Epoch: 1 [2325000/60000 (66%)]\tLoss: 0.019387\n",
            "Train Epoch: 1 [2343750/60000 (67%)]\tLoss: 0.014943\n",
            "Train Epoch: 1 [2362500/60000 (67%)]\tLoss: 0.099418\n",
            "Train Epoch: 1 [2381250/60000 (68%)]\tLoss: 0.020239\n",
            "Train Epoch: 1 [2400000/60000 (68%)]\tLoss: 0.039820\n",
            "Train Epoch: 1 [2418750/60000 (69%)]\tLoss: 0.064927\n",
            "Train Epoch: 1 [2437500/60000 (69%)]\tLoss: 0.105779\n",
            "Train Epoch: 1 [2456250/60000 (70%)]\tLoss: 0.045149\n",
            "Train Epoch: 1 [2475000/60000 (70%)]\tLoss: 0.121769\n",
            "Train Epoch: 1 [2493750/60000 (71%)]\tLoss: 0.061265\n",
            "Train Epoch: 1 [2512500/60000 (71%)]\tLoss: 0.040354\n",
            "Train Epoch: 1 [2531250/60000 (72%)]\tLoss: 0.001146\n",
            "Train Epoch: 1 [2550000/60000 (73%)]\tLoss: 0.004910\n",
            "Train Epoch: 1 [2568750/60000 (73%)]\tLoss: 0.084031\n",
            "Train Epoch: 1 [2587500/60000 (74%)]\tLoss: 0.016669\n",
            "Train Epoch: 1 [2606250/60000 (74%)]\tLoss: 0.110607\n",
            "Train Epoch: 1 [2625000/60000 (75%)]\tLoss: 0.007575\n",
            "Train Epoch: 1 [2643750/60000 (75%)]\tLoss: 0.089450\n",
            "Train Epoch: 1 [2662500/60000 (76%)]\tLoss: 0.003739\n",
            "Train Epoch: 1 [2681250/60000 (76%)]\tLoss: 0.092195\n",
            "Train Epoch: 1 [2700000/60000 (77%)]\tLoss: 0.243006\n",
            "Train Epoch: 1 [2718750/60000 (77%)]\tLoss: 0.008927\n",
            "Train Epoch: 1 [2737500/60000 (78%)]\tLoss: 0.074643\n",
            "Train Epoch: 1 [2756250/60000 (78%)]\tLoss: 0.006275\n",
            "Train Epoch: 1 [2775000/60000 (79%)]\tLoss: 0.158664\n",
            "Train Epoch: 1 [2793750/60000 (79%)]\tLoss: 0.021931\n",
            "Train Epoch: 1 [2812500/60000 (80%)]\tLoss: 0.210821\n",
            "Train Epoch: 1 [2831250/60000 (81%)]\tLoss: 0.065075\n",
            "Train Epoch: 1 [2850000/60000 (81%)]\tLoss: 0.147222\n",
            "Train Epoch: 1 [2868750/60000 (82%)]\tLoss: 0.005307\n",
            "Train Epoch: 1 [2887500/60000 (82%)]\tLoss: 0.030185\n",
            "Train Epoch: 1 [2906250/60000 (83%)]\tLoss: 0.047130\n",
            "Train Epoch: 1 [2925000/60000 (83%)]\tLoss: 0.121522\n",
            "Train Epoch: 1 [2943750/60000 (84%)]\tLoss: 0.003715\n",
            "Train Epoch: 1 [2962500/60000 (84%)]\tLoss: 0.012967\n",
            "Train Epoch: 1 [2981250/60000 (85%)]\tLoss: 0.191693\n",
            "Train Epoch: 1 [3000000/60000 (85%)]\tLoss: 0.003518\n",
            "Train Epoch: 1 [3018750/60000 (86%)]\tLoss: 0.001823\n",
            "Train Epoch: 1 [3037500/60000 (86%)]\tLoss: 0.000342\n",
            "Train Epoch: 1 [3056250/60000 (87%)]\tLoss: 0.000220\n",
            "Train Epoch: 1 [3075000/60000 (87%)]\tLoss: 0.003634\n",
            "Train Epoch: 1 [3093750/60000 (88%)]\tLoss: 0.123125\n",
            "Train Epoch: 1 [3112500/60000 (89%)]\tLoss: 0.079953\n",
            "Train Epoch: 1 [3131250/60000 (89%)]\tLoss: 0.050819\n",
            "Train Epoch: 1 [3150000/60000 (90%)]\tLoss: 0.023015\n",
            "Train Epoch: 1 [3168750/60000 (90%)]\tLoss: 0.022179\n",
            "Train Epoch: 1 [3187500/60000 (91%)]\tLoss: 0.037182\n",
            "Train Epoch: 1 [3206250/60000 (91%)]\tLoss: 0.240238\n",
            "Train Epoch: 1 [3225000/60000 (92%)]\tLoss: 0.032032\n",
            "Train Epoch: 1 [3243750/60000 (92%)]\tLoss: 0.035766\n",
            "Train Epoch: 1 [3262500/60000 (93%)]\tLoss: 0.062526\n",
            "Train Epoch: 1 [3281250/60000 (93%)]\tLoss: 0.014437\n",
            "Train Epoch: 1 [3300000/60000 (94%)]\tLoss: 0.014858\n",
            "Train Epoch: 1 [3318750/60000 (94%)]\tLoss: 0.008020\n",
            "Train Epoch: 1 [3337500/60000 (95%)]\tLoss: 0.035251\n",
            "Train Epoch: 1 [3356250/60000 (95%)]\tLoss: 0.105082\n",
            "Train Epoch: 1 [3375000/60000 (96%)]\tLoss: 0.005602\n",
            "Train Epoch: 1 [3393750/60000 (97%)]\tLoss: 0.026189\n",
            "Train Epoch: 1 [3412500/60000 (97%)]\tLoss: 0.110610\n",
            "Train Epoch: 1 [3431250/60000 (98%)]\tLoss: 0.015032\n",
            "Train Epoch: 1 [3450000/60000 (98%)]\tLoss: 0.004217\n",
            "Train Epoch: 1 [3468750/60000 (99%)]\tLoss: 0.011224\n",
            "Train Epoch: 1 [3487500/60000 (99%)]\tLoss: 0.490251\n",
            "Train Epoch: 1 [3506250/60000 (100%)]\tLoss: 0.014565\n",
            "\n",
            "Test set: Average loss: 0.0404, Accuracy: 9861/10000 (99%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.085613\n",
            "Train Epoch: 2 [18750/60000 (1%)]\tLoss: 0.088783\n",
            "Train Epoch: 2 [37500/60000 (1%)]\tLoss: 0.002891\n",
            "Train Epoch: 2 [56250/60000 (2%)]\tLoss: 0.067938\n",
            "Train Epoch: 2 [75000/60000 (2%)]\tLoss: 0.001110\n",
            "Train Epoch: 2 [93750/60000 (3%)]\tLoss: 0.006097\n",
            "Train Epoch: 2 [112500/60000 (3%)]\tLoss: 0.023898\n",
            "Train Epoch: 2 [131250/60000 (4%)]\tLoss: 0.005520\n",
            "Train Epoch: 2 [150000/60000 (4%)]\tLoss: 0.002528\n",
            "Train Epoch: 2 [168750/60000 (5%)]\tLoss: 0.002205\n",
            "Train Epoch: 2 [187500/60000 (5%)]\tLoss: 0.063298\n",
            "Train Epoch: 2 [206250/60000 (6%)]\tLoss: 0.011004\n",
            "Train Epoch: 2 [225000/60000 (6%)]\tLoss: 0.175343\n",
            "Train Epoch: 2 [243750/60000 (7%)]\tLoss: 0.122220\n",
            "Train Epoch: 2 [262500/60000 (7%)]\tLoss: 0.149350\n",
            "Train Epoch: 2 [281250/60000 (8%)]\tLoss: 0.081666\n",
            "Train Epoch: 2 [300000/60000 (9%)]\tLoss: 0.071792\n",
            "Train Epoch: 2 [318750/60000 (9%)]\tLoss: 0.063043\n",
            "Train Epoch: 2 [337500/60000 (10%)]\tLoss: 0.159488\n",
            "Train Epoch: 2 [356250/60000 (10%)]\tLoss: 0.003927\n",
            "Train Epoch: 2 [375000/60000 (11%)]\tLoss: 0.029085\n",
            "Train Epoch: 2 [393750/60000 (11%)]\tLoss: 0.021426\n",
            "Train Epoch: 2 [412500/60000 (12%)]\tLoss: 0.006094\n",
            "Train Epoch: 2 [431250/60000 (12%)]\tLoss: 0.002194\n",
            "Train Epoch: 2 [450000/60000 (13%)]\tLoss: 0.000790\n",
            "Train Epoch: 2 [468750/60000 (13%)]\tLoss: 0.014759\n",
            "Train Epoch: 2 [487500/60000 (14%)]\tLoss: 0.051293\n",
            "Train Epoch: 2 [506250/60000 (14%)]\tLoss: 0.036995\n",
            "Train Epoch: 2 [525000/60000 (15%)]\tLoss: 0.102995\n",
            "Train Epoch: 2 [543750/60000 (15%)]\tLoss: 0.008003\n",
            "Train Epoch: 2 [562500/60000 (16%)]\tLoss: 0.061177\n",
            "Train Epoch: 2 [581250/60000 (17%)]\tLoss: 0.009002\n",
            "Train Epoch: 2 [600000/60000 (17%)]\tLoss: 0.018735\n",
            "Train Epoch: 2 [618750/60000 (18%)]\tLoss: 0.024019\n",
            "Train Epoch: 2 [637500/60000 (18%)]\tLoss: 0.143573\n",
            "Train Epoch: 2 [656250/60000 (19%)]\tLoss: 0.008988\n",
            "Train Epoch: 2 [675000/60000 (19%)]\tLoss: 0.053431\n",
            "Train Epoch: 2 [693750/60000 (20%)]\tLoss: 0.028004\n",
            "Train Epoch: 2 [712500/60000 (20%)]\tLoss: 0.089770\n",
            "Train Epoch: 2 [731250/60000 (21%)]\tLoss: 0.029704\n",
            "Train Epoch: 2 [750000/60000 (21%)]\tLoss: 0.026973\n",
            "Train Epoch: 2 [768750/60000 (22%)]\tLoss: 0.090111\n",
            "Train Epoch: 2 [787500/60000 (22%)]\tLoss: 0.004372\n",
            "Train Epoch: 2 [806250/60000 (23%)]\tLoss: 0.007344\n",
            "Train Epoch: 2 [825000/60000 (23%)]\tLoss: 0.146863\n",
            "Train Epoch: 2 [843750/60000 (24%)]\tLoss: 0.134853\n",
            "Train Epoch: 2 [862500/60000 (25%)]\tLoss: 0.078059\n",
            "Train Epoch: 2 [881250/60000 (25%)]\tLoss: 0.021528\n",
            "Train Epoch: 2 [900000/60000 (26%)]\tLoss: 0.031103\n",
            "Train Epoch: 2 [918750/60000 (26%)]\tLoss: 0.301524\n",
            "Train Epoch: 2 [937500/60000 (27%)]\tLoss: 0.007193\n",
            "Train Epoch: 2 [956250/60000 (27%)]\tLoss: 0.019609\n",
            "Train Epoch: 2 [975000/60000 (28%)]\tLoss: 0.006555\n",
            "Train Epoch: 2 [993750/60000 (28%)]\tLoss: 0.126829\n",
            "Train Epoch: 2 [1012500/60000 (29%)]\tLoss: 0.013755\n",
            "Train Epoch: 2 [1031250/60000 (29%)]\tLoss: 0.014794\n",
            "Train Epoch: 2 [1050000/60000 (30%)]\tLoss: 0.021189\n",
            "Train Epoch: 2 [1068750/60000 (30%)]\tLoss: 0.006462\n",
            "Train Epoch: 2 [1087500/60000 (31%)]\tLoss: 0.001837\n",
            "Train Epoch: 2 [1106250/60000 (31%)]\tLoss: 0.270557\n",
            "Train Epoch: 2 [1125000/60000 (32%)]\tLoss: 0.028867\n",
            "Train Epoch: 2 [1143750/60000 (33%)]\tLoss: 0.004941\n",
            "Train Epoch: 2 [1162500/60000 (33%)]\tLoss: 0.310386\n",
            "Train Epoch: 2 [1181250/60000 (34%)]\tLoss: 0.014566\n",
            "Train Epoch: 2 [1200000/60000 (34%)]\tLoss: 0.028828\n",
            "Train Epoch: 2 [1218750/60000 (35%)]\tLoss: 0.019536\n",
            "Train Epoch: 2 [1237500/60000 (35%)]\tLoss: 0.004077\n",
            "Train Epoch: 2 [1256250/60000 (36%)]\tLoss: 0.028836\n",
            "Train Epoch: 2 [1275000/60000 (36%)]\tLoss: 0.177811\n",
            "Train Epoch: 2 [1293750/60000 (37%)]\tLoss: 0.177418\n",
            "Train Epoch: 2 [1312500/60000 (37%)]\tLoss: 0.020701\n",
            "Train Epoch: 2 [1331250/60000 (38%)]\tLoss: 0.003273\n",
            "Train Epoch: 2 [1350000/60000 (38%)]\tLoss: 0.013397\n",
            "Train Epoch: 2 [1368750/60000 (39%)]\tLoss: 0.002218\n",
            "Train Epoch: 2 [1387500/60000 (39%)]\tLoss: 0.068708\n",
            "Train Epoch: 2 [1406250/60000 (40%)]\tLoss: 0.003341\n",
            "Train Epoch: 2 [1425000/60000 (41%)]\tLoss: 0.008204\n",
            "Train Epoch: 2 [1443750/60000 (41%)]\tLoss: 0.003767\n",
            "Train Epoch: 2 [1462500/60000 (42%)]\tLoss: 0.010329\n",
            "Train Epoch: 2 [1481250/60000 (42%)]\tLoss: 0.000382\n",
            "Train Epoch: 2 [1500000/60000 (43%)]\tLoss: 0.146087\n",
            "Train Epoch: 2 [1518750/60000 (43%)]\tLoss: 0.000281\n",
            "Train Epoch: 2 [1537500/60000 (44%)]\tLoss: 0.002602\n",
            "Train Epoch: 2 [1556250/60000 (44%)]\tLoss: 0.053051\n",
            "Train Epoch: 2 [1575000/60000 (45%)]\tLoss: 0.005298\n",
            "Train Epoch: 2 [1593750/60000 (45%)]\tLoss: 0.035330\n",
            "Train Epoch: 2 [1612500/60000 (46%)]\tLoss: 0.007923\n",
            "Train Epoch: 2 [1631250/60000 (46%)]\tLoss: 0.001253\n",
            "Train Epoch: 2 [1650000/60000 (47%)]\tLoss: 0.001018\n",
            "Train Epoch: 2 [1668750/60000 (47%)]\tLoss: 0.000438\n",
            "Train Epoch: 2 [1687500/60000 (48%)]\tLoss: 0.058058\n",
            "Train Epoch: 2 [1706250/60000 (49%)]\tLoss: 0.009585\n",
            "Train Epoch: 2 [1725000/60000 (49%)]\tLoss: 0.004030\n",
            "Train Epoch: 2 [1743750/60000 (50%)]\tLoss: 0.119897\n",
            "Train Epoch: 2 [1762500/60000 (50%)]\tLoss: 0.010228\n",
            "Train Epoch: 2 [1781250/60000 (51%)]\tLoss: 0.035205\n",
            "Train Epoch: 2 [1800000/60000 (51%)]\tLoss: 0.000127\n",
            "Train Epoch: 2 [1818750/60000 (52%)]\tLoss: 0.039073\n",
            "Train Epoch: 2 [1837500/60000 (52%)]\tLoss: 0.280636\n",
            "Train Epoch: 2 [1856250/60000 (53%)]\tLoss: 0.005392\n",
            "Train Epoch: 2 [1875000/60000 (53%)]\tLoss: 0.092828\n",
            "Train Epoch: 2 [1893750/60000 (54%)]\tLoss: 0.071750\n",
            "Train Epoch: 2 [1912500/60000 (54%)]\tLoss: 0.251504\n",
            "Train Epoch: 2 [1931250/60000 (55%)]\tLoss: 0.145194\n",
            "Train Epoch: 2 [1950000/60000 (55%)]\tLoss: 0.001087\n",
            "Train Epoch: 2 [1968750/60000 (56%)]\tLoss: 0.035407\n",
            "Train Epoch: 2 [1987500/60000 (57%)]\tLoss: 0.002181\n",
            "Train Epoch: 2 [2006250/60000 (57%)]\tLoss: 0.106895\n",
            "Train Epoch: 2 [2025000/60000 (58%)]\tLoss: 0.100127\n",
            "Train Epoch: 2 [2043750/60000 (58%)]\tLoss: 0.000433\n",
            "Train Epoch: 2 [2062500/60000 (59%)]\tLoss: 0.107943\n",
            "Train Epoch: 2 [2081250/60000 (59%)]\tLoss: 0.001216\n",
            "Train Epoch: 2 [2100000/60000 (60%)]\tLoss: 0.021252\n",
            "Train Epoch: 2 [2118750/60000 (60%)]\tLoss: 0.090078\n",
            "Train Epoch: 2 [2137500/60000 (61%)]\tLoss: 0.061623\n",
            "Train Epoch: 2 [2156250/60000 (61%)]\tLoss: 0.000780\n",
            "Train Epoch: 2 [2175000/60000 (62%)]\tLoss: 0.087523\n",
            "Train Epoch: 2 [2193750/60000 (62%)]\tLoss: 0.033209\n",
            "Train Epoch: 2 [2212500/60000 (63%)]\tLoss: 0.003310\n",
            "Train Epoch: 2 [2231250/60000 (63%)]\tLoss: 0.167817\n",
            "Train Epoch: 2 [2250000/60000 (64%)]\tLoss: 0.021982\n",
            "Train Epoch: 2 [2268750/60000 (65%)]\tLoss: 0.156228\n",
            "Train Epoch: 2 [2287500/60000 (65%)]\tLoss: 0.027831\n",
            "Train Epoch: 2 [2306250/60000 (66%)]\tLoss: 0.068731\n",
            "Train Epoch: 2 [2325000/60000 (66%)]\tLoss: 0.006306\n",
            "Train Epoch: 2 [2343750/60000 (67%)]\tLoss: 0.036921\n",
            "Train Epoch: 2 [2362500/60000 (67%)]\tLoss: 0.001235\n",
            "Train Epoch: 2 [2381250/60000 (68%)]\tLoss: 0.015561\n",
            "Train Epoch: 2 [2400000/60000 (68%)]\tLoss: 0.019990\n",
            "Train Epoch: 2 [2418750/60000 (69%)]\tLoss: 0.172140\n",
            "Train Epoch: 2 [2437500/60000 (69%)]\tLoss: 0.042907\n",
            "Train Epoch: 2 [2456250/60000 (70%)]\tLoss: 0.038239\n",
            "Train Epoch: 2 [2475000/60000 (70%)]\tLoss: 0.019447\n",
            "Train Epoch: 2 [2493750/60000 (71%)]\tLoss: 0.042037\n",
            "Train Epoch: 2 [2512500/60000 (71%)]\tLoss: 0.068653\n",
            "Train Epoch: 2 [2531250/60000 (72%)]\tLoss: 0.020542\n",
            "Train Epoch: 2 [2550000/60000 (73%)]\tLoss: 0.020310\n",
            "Train Epoch: 2 [2568750/60000 (73%)]\tLoss: 0.018917\n",
            "Train Epoch: 2 [2587500/60000 (74%)]\tLoss: 0.025549\n",
            "Train Epoch: 2 [2606250/60000 (74%)]\tLoss: 0.005192\n",
            "Train Epoch: 2 [2625000/60000 (75%)]\tLoss: 0.001561\n",
            "Train Epoch: 2 [2643750/60000 (75%)]\tLoss: 0.005792\n",
            "Train Epoch: 2 [2662500/60000 (76%)]\tLoss: 0.003449\n",
            "Train Epoch: 2 [2681250/60000 (76%)]\tLoss: 0.008769\n",
            "Train Epoch: 2 [2700000/60000 (77%)]\tLoss: 0.031928\n",
            "Train Epoch: 2 [2718750/60000 (77%)]\tLoss: 0.091973\n",
            "Train Epoch: 2 [2737500/60000 (78%)]\tLoss: 0.000357\n",
            "Train Epoch: 2 [2756250/60000 (78%)]\tLoss: 0.016530\n",
            "Train Epoch: 2 [2775000/60000 (79%)]\tLoss: 0.002874\n",
            "Train Epoch: 2 [2793750/60000 (79%)]\tLoss: 0.012137\n",
            "Train Epoch: 2 [2812500/60000 (80%)]\tLoss: 0.003817\n",
            "Train Epoch: 2 [2831250/60000 (81%)]\tLoss: 0.132022\n",
            "Train Epoch: 2 [2850000/60000 (81%)]\tLoss: 0.085471\n",
            "Train Epoch: 2 [2868750/60000 (82%)]\tLoss: 0.003710\n",
            "Train Epoch: 2 [2887500/60000 (82%)]\tLoss: 0.001872\n",
            "Train Epoch: 2 [2906250/60000 (83%)]\tLoss: 0.008196\n",
            "Train Epoch: 2 [2925000/60000 (83%)]\tLoss: 0.094527\n",
            "Train Epoch: 2 [2943750/60000 (84%)]\tLoss: 0.013122\n",
            "Train Epoch: 2 [2962500/60000 (84%)]\tLoss: 0.009839\n",
            "Train Epoch: 2 [2981250/60000 (85%)]\tLoss: 0.198991\n",
            "Train Epoch: 2 [3000000/60000 (85%)]\tLoss: 0.003584\n",
            "Train Epoch: 2 [3018750/60000 (86%)]\tLoss: 0.052395\n",
            "Train Epoch: 2 [3037500/60000 (86%)]\tLoss: 0.004044\n",
            "Train Epoch: 2 [3056250/60000 (87%)]\tLoss: 0.006257\n",
            "Train Epoch: 2 [3075000/60000 (87%)]\tLoss: 0.049334\n",
            "Train Epoch: 2 [3093750/60000 (88%)]\tLoss: 0.032320\n",
            "Train Epoch: 2 [3112500/60000 (89%)]\tLoss: 0.000517\n",
            "Train Epoch: 2 [3131250/60000 (89%)]\tLoss: 0.058346\n",
            "Train Epoch: 2 [3150000/60000 (90%)]\tLoss: 0.003481\n",
            "Train Epoch: 2 [3168750/60000 (90%)]\tLoss: 0.005970\n",
            "Train Epoch: 2 [3187500/60000 (91%)]\tLoss: 0.004663\n",
            "Train Epoch: 2 [3206250/60000 (91%)]\tLoss: 0.156564\n",
            "Train Epoch: 2 [3225000/60000 (92%)]\tLoss: 0.370532\n",
            "Train Epoch: 2 [3243750/60000 (92%)]\tLoss: 0.007467\n",
            "Train Epoch: 2 [3262500/60000 (93%)]\tLoss: 0.014607\n",
            "Train Epoch: 2 [3281250/60000 (93%)]\tLoss: 0.103824\n",
            "Train Epoch: 2 [3300000/60000 (94%)]\tLoss: 0.075705\n",
            "Train Epoch: 2 [3318750/60000 (94%)]\tLoss: 0.014172\n",
            "Train Epoch: 2 [3337500/60000 (95%)]\tLoss: 0.008904\n",
            "Train Epoch: 2 [3356250/60000 (95%)]\tLoss: 0.031507\n",
            "Train Epoch: 2 [3375000/60000 (96%)]\tLoss: 0.021774\n",
            "Train Epoch: 2 [3393750/60000 (97%)]\tLoss: 0.039357\n",
            "Train Epoch: 2 [3412500/60000 (97%)]\tLoss: 0.023605\n",
            "Train Epoch: 2 [3431250/60000 (98%)]\tLoss: 0.176338\n",
            "Train Epoch: 2 [3450000/60000 (98%)]\tLoss: 0.014505\n",
            "Train Epoch: 2 [3468750/60000 (99%)]\tLoss: 0.023963\n",
            "Train Epoch: 2 [3487500/60000 (99%)]\tLoss: 0.045599\n",
            "Train Epoch: 2 [3506250/60000 (100%)]\tLoss: 0.002125\n",
            "\n",
            "Test set: Average loss: 0.0395, Accuracy: 9872/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 훈련 후 테스트셋을 통해 성능도 검증했으니 샘플 이미지에서 추론이 맞는지 확인\n",
        "test_samples = enumerate(test_dataloader)\n",
        "b_i, (sample_data, sample_targets) = next(test_samples)\n",
        "\n",
        "# 확률이 가장 높은 클래스(max)를 선택. model(sample_data)[1]은 숫자 분류 결과 배열.\n",
        "print(f\"model prediction : {model(sample_data).data.max(1)[1][0]}\")\n",
        "print(f\"Ground truth : {sample_targets[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfkldWN9c-M4",
        "outputId": "0b7def26-6d8b-4df1-a9f0-bd84f286c3c9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model prediction : 0\n",
            "Ground truth : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN과 LSTM 결합하기"
      ],
      "metadata": {
        "id": "fqcar5Xwg4Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mbczLVpXhLyD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}