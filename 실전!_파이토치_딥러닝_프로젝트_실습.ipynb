{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJF/XWPd1RWrEwVfwQjVHO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#MNIST 예시"
      ],
      "metadata": {
        "id": "EoGe7_Jg6mfD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgJu9Eq96l03"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# 신경망 구축에 필요한 여러 매서드를 담은 torch.nn\n",
        "import torch.nn as nn\n",
        "# torch.nn의 모든 함수를 포함, 손실/활성화/풀링/합성곱/선형 및 기타 신경망 함수 가 포함됨\n",
        "import torch.nn.functional as F\n",
        "# 최적화(신경망의 가중치,매개변수 조정을 위해 오차를 역전파 하는 과정) 모듈이 담김\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "  \"\"\"\n",
        "  kernel_size : 보통 홀수. 너무 작으면 픽셀을 처리하는 kernel이 이웃 픽셀의 정보를 가지지 못한다.너무 크면 이미지 내에서 정밀하지\n",
        "                않은 특징을 얻게 됨. 작은 kernel_size의 많은 layer를 쓰면 네트워크가 깊어지고, 더 복잡한 특징 학습 가능.\n",
        "\n",
        "  feature map : 이미지 데이터에서 픽셀 정보를 담고 있는 channel(차원)을 의미. 이미지에서 더 많은 특징을 추출하려거든 channel을 크게\n",
        "                하면 된다.\n",
        "\n",
        "  input shape이 28x28x1임\n",
        "\n",
        "  Conv2d(input_channel, output_channel, kernel_size, stride)\n",
        "  \"\"\"\n",
        "  # 각 layer의 뉴런개수 및 layer들 정의\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "\n",
        "    # 합성곱 layer\n",
        "    self.cn1 = nn.Conv2d(1,16,3,1)\n",
        "    self.cn2 = nn.Conv2d(16,32,3,1)\n",
        "    # 드롭아웃 layer\n",
        "    self.dp1 = nn.Dropout(0.1)\n",
        "    self.dp2 = nn.Dropout(0.25)\n",
        "    # fully-connected(fc) layer, 4608=12x12x32\n",
        "    self.fc1 = nn.Linear(4608, 64)\n",
        "    # 최종 출력은 10개 클래스 중 하나\n",
        "    self.fc2 = nn.Linear(64,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.cn1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.cn2(x)\n",
        "    x = F.relu(x)\n",
        "    # kernel size가 2x2\n",
        "    x = F.max_pool2d(x,2)\n",
        "    x = self.dp1(x)\n",
        "    # 1차원 백터로 평면화\n",
        "    x = torch.flatten(x,1)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dp2(x)\n",
        "    x = self.fc2(x)\n",
        "    # 모델의 예측을 out에 담음\n",
        "    out = F.log_softmax(x, dim=1)\n",
        "    return out"
      ],
      "metadata": {
        "id": "DYydgv6t6zdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 루틴 정의\n",
        "def train(model, device, train_dataloader, optim, epoch):\n",
        "  # 모델 훈련\n",
        "  model.train()\n",
        "  # batch 단위로 반복\n",
        "  for b_i, (X,y) in enumerate(train_dataloader):\n",
        "    # 주어진 로컬 메모리에 데이터셋 사본 만듦\n",
        "    X,y = X.to(device), y.to(device)\n",
        "    # 이전에 계산했던 gradient를 초기화(이전 값들은 이미 이전단계의 파라미터 수정할 때 쓰였으니까 노필요)\n",
        "    optim.zero_grad()\n",
        "    # 주어진 입력데이터를 활용하여 모델 예측 실행\n",
        "    pred = model(X)\n",
        "    # negative log liklihood, 모델예측값과 실제값 사이의 손실 계산\n",
        "    loss = F.nll_loss(pred, y)\n",
        "    # 역전파, 자동미분됨\n",
        "    loss.backward()\n",
        "    # 가중치 조정\n",
        "    optim.step()\n",
        "    if b_i % 10 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, b_i * len(train_dataloader), len(train_dataloader.dataset),100. * b_i / len(train_dataloader), loss.item()))"
      ],
      "metadata": {
        "id": "1eWbfHJuOwBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 루틴 정의\n",
        "def test(model, device, test_dataloader):\n",
        "  \"\"\"\n",
        "  torch.no_grad : 추론(평가) 과정에서 사용하는 autograd 끄는 함수. 모델이 파라미터 업데이트 안 하고(즉, 학습X) 단순히 예측/추론만 함.\n",
        "\n",
        "  평균적인 손실(오차) 정도를 구하기 위해 loss를 합한다.\n",
        "  \"\"\"\n",
        "\n",
        "  # 모델 성능 평가\n",
        "  model.eval()\n",
        "  loss = 0\n",
        "  success = 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataloader:\n",
        "      X,y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      # 배치별 손실의 합(옵티마이저X->모델 가중치 조정X->모델평가를 위해 배치 단위로 오차 합함)\n",
        "      loss += F.nll_loss(pred, y, reduction='sum').item()\n",
        "      pred = pred.argmax(dim=1, keepdim=True)\n",
        "      success += pred.eq(y.view_as(pred)).sum().item()\n",
        "    loss /= len(test_dataloader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(loss, success, len(test_dataloader.dataset),100. * success / len(test_dataloader.dataset)))"
      ],
      "metadata": {
        "id": "BYHX8xVWStHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "torch.DataLoader : DataLoader에 입력되는 dataset을 모델에 배치입력하기 쉽게 만들어주는 모듈(추가적인 동작이 필요할 경우 DataSet을 상속받는\n",
        "                   사용자 정의 DataSet을 만들어 그 DataSet을 DataLoader에 넣기도 함). 그래서 Dataset갹채에서 배치단위 데이터를 가져온 걸 반환.\n",
        "\n",
        "transform 파리미터는 MNIST 이미지 데이터에 대한 전처리 과정을 정의.\n",
        "\n",
        "transforms.Compose() 괄호 안에 들어가는 변환함수들을 담은 리스트는 순차적으로 입력데이터에 적용됨\n",
        "\n",
        "transforms.Normalize((mean, std))\n",
        "\n",
        "batch_size : 한번에 모델에 입력되는 데이터 묶음. 총 1000개의 이미지데이터가 있다면 batch_size가 32이라면 한 번 학습 시에\n",
        "             32개의 이미지(데이터,혹은 샘플)가 쓰인다는 것. 그리고 이때 1000/32 약 32개의 배치(데이터묶음)가 생기는데\n",
        "             마지막 배치의 경우 (1000-31)*32로 8개의 이미지데이터가 담김\n",
        "# # 훈련 데이터 불러오기\n",
        "# train_dataloader = torch.utils.data.DataLoader(datasets.MNIST(\"../data\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor(),\n",
        "#                                                                                                                                   transforms.ToTensor(),\n",
        "#                                                                                                                                   transforms.Normalize((0.1302,),(0.3069))])),\n",
        "#                                                batch_size=32, shuffle=True)\n",
        "# # 성능 평가를 위한 데이터 불러오기\n",
        "# test_dataloader = torch.utils.data.DataLoader(datasets.MNIST(\"../data\", train=False, transform=transforms.Compose([transforms.ToTensor(),\n",
        "#                                                                                                                    transforms.Normalize((0.1302,),(0.3069))])),\n",
        "#                                                batch_size=500, shuffle=True)\n",
        "\"\"\"\n",
        "\n",
        "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                    transform=transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset1,batch_size=32, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset2, batch_size=500, shuffle=True)"
      ],
      "metadata": {
        "id": "wW51u2_zVRcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 무작위성X/재현가능성 위해 시드값 설정\n",
        "torch.manual_seed(0)\n",
        "# 연산 수행할 장치 지정\n",
        "device = torch.device(\"cpu\")\n",
        "# 모델 객체 생성\n",
        "model = ConvNet()\n",
        "# 옵티마이저 객체 생성\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
      ],
      "metadata": {
        "id": "da_7-URMYG_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 실제로 훈련,테스트\n",
        "# %capture\n",
        "for epoch in range(1,3):\n",
        "  train(model, device, train_dataloader, optimizer, epoch)\n",
        "  test(model, device, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InMw35Y0Yic3",
        "outputId": "a38adf5b-909f-4fee-8dc3-6f329d66a66c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.311855\n",
            "Train Epoch: 1 [18750/60000 (1%)]\tLoss: 1.873572\n",
            "Train Epoch: 1 [37500/60000 (1%)]\tLoss: 1.161529\n",
            "Train Epoch: 1 [56250/60000 (2%)]\tLoss: 0.849931\n",
            "Train Epoch: 1 [75000/60000 (2%)]\tLoss: 0.852791\n",
            "Train Epoch: 1 [93750/60000 (3%)]\tLoss: 0.439481\n",
            "Train Epoch: 1 [112500/60000 (3%)]\tLoss: 0.480759\n",
            "Train Epoch: 1 [131250/60000 (4%)]\tLoss: 0.413792\n",
            "Train Epoch: 1 [150000/60000 (4%)]\tLoss: 0.409818\n",
            "Train Epoch: 1 [168750/60000 (5%)]\tLoss: 0.342001\n",
            "Train Epoch: 1 [187500/60000 (5%)]\tLoss: 0.471768\n",
            "Train Epoch: 1 [206250/60000 (6%)]\tLoss: 0.284015\n",
            "Train Epoch: 1 [225000/60000 (6%)]\tLoss: 0.418780\n",
            "Train Epoch: 1 [243750/60000 (7%)]\tLoss: 0.566111\n",
            "Train Epoch: 1 [262500/60000 (7%)]\tLoss: 0.413485\n",
            "Train Epoch: 1 [281250/60000 (8%)]\tLoss: 0.603133\n",
            "Train Epoch: 1 [300000/60000 (9%)]\tLoss: 0.130813\n",
            "Train Epoch: 1 [318750/60000 (9%)]\tLoss: 0.292469\n",
            "Train Epoch: 1 [337500/60000 (10%)]\tLoss: 0.090156\n",
            "Train Epoch: 1 [356250/60000 (10%)]\tLoss: 0.190347\n",
            "Train Epoch: 1 [375000/60000 (11%)]\tLoss: 0.270573\n",
            "Train Epoch: 1 [393750/60000 (11%)]\tLoss: 0.069167\n",
            "Train Epoch: 1 [412500/60000 (12%)]\tLoss: 0.400788\n",
            "Train Epoch: 1 [431250/60000 (12%)]\tLoss: 0.030892\n",
            "Train Epoch: 1 [450000/60000 (13%)]\tLoss: 0.249858\n",
            "Train Epoch: 1 [468750/60000 (13%)]\tLoss: 0.258238\n",
            "Train Epoch: 1 [487500/60000 (14%)]\tLoss: 0.282275\n",
            "Train Epoch: 1 [506250/60000 (14%)]\tLoss: 0.224871\n",
            "Train Epoch: 1 [525000/60000 (15%)]\tLoss: 0.093075\n",
            "Train Epoch: 1 [543750/60000 (15%)]\tLoss: 0.192811\n",
            "Train Epoch: 1 [562500/60000 (16%)]\tLoss: 0.254926\n",
            "Train Epoch: 1 [581250/60000 (17%)]\tLoss: 0.255227\n",
            "Train Epoch: 1 [600000/60000 (17%)]\tLoss: 0.274714\n",
            "Train Epoch: 1 [618750/60000 (18%)]\tLoss: 0.486867\n",
            "Train Epoch: 1 [637500/60000 (18%)]\tLoss: 0.090187\n",
            "Train Epoch: 1 [656250/60000 (19%)]\tLoss: 0.117295\n",
            "Train Epoch: 1 [675000/60000 (19%)]\tLoss: 0.159630\n",
            "Train Epoch: 1 [693750/60000 (20%)]\tLoss: 0.081932\n",
            "Train Epoch: 1 [712500/60000 (20%)]\tLoss: 0.225531\n",
            "Train Epoch: 1 [731250/60000 (21%)]\tLoss: 0.045010\n",
            "Train Epoch: 1 [750000/60000 (21%)]\tLoss: 0.127294\n",
            "Train Epoch: 1 [768750/60000 (22%)]\tLoss: 0.086787\n",
            "Train Epoch: 1 [787500/60000 (22%)]\tLoss: 0.095210\n",
            "Train Epoch: 1 [806250/60000 (23%)]\tLoss: 0.093002\n",
            "Train Epoch: 1 [825000/60000 (23%)]\tLoss: 0.035825\n",
            "Train Epoch: 1 [843750/60000 (24%)]\tLoss: 0.129808\n",
            "Train Epoch: 1 [862500/60000 (25%)]\tLoss: 0.159104\n",
            "Train Epoch: 1 [881250/60000 (25%)]\tLoss: 0.336953\n",
            "Train Epoch: 1 [900000/60000 (26%)]\tLoss: 0.218910\n",
            "Train Epoch: 1 [918750/60000 (26%)]\tLoss: 0.100763\n",
            "Train Epoch: 1 [937500/60000 (27%)]\tLoss: 0.149333\n",
            "Train Epoch: 1 [956250/60000 (27%)]\tLoss: 0.149923\n",
            "Train Epoch: 1 [975000/60000 (28%)]\tLoss: 0.037672\n",
            "Train Epoch: 1 [993750/60000 (28%)]\tLoss: 0.298493\n",
            "Train Epoch: 1 [1012500/60000 (29%)]\tLoss: 0.079385\n",
            "Train Epoch: 1 [1031250/60000 (29%)]\tLoss: 0.014785\n",
            "Train Epoch: 1 [1050000/60000 (30%)]\tLoss: 0.018355\n",
            "Train Epoch: 1 [1068750/60000 (30%)]\tLoss: 0.054329\n",
            "Train Epoch: 1 [1087500/60000 (31%)]\tLoss: 0.108350\n",
            "Train Epoch: 1 [1106250/60000 (31%)]\tLoss: 0.297600\n",
            "Train Epoch: 1 [1125000/60000 (32%)]\tLoss: 0.084368\n",
            "Train Epoch: 1 [1143750/60000 (33%)]\tLoss: 0.067797\n",
            "Train Epoch: 1 [1162500/60000 (33%)]\tLoss: 0.116537\n",
            "Train Epoch: 1 [1181250/60000 (34%)]\tLoss: 0.200433\n",
            "Train Epoch: 1 [1200000/60000 (34%)]\tLoss: 0.069181\n",
            "Train Epoch: 1 [1218750/60000 (35%)]\tLoss: 0.167293\n",
            "Train Epoch: 1 [1237500/60000 (35%)]\tLoss: 0.267976\n",
            "Train Epoch: 1 [1256250/60000 (36%)]\tLoss: 0.109871\n",
            "Train Epoch: 1 [1275000/60000 (36%)]\tLoss: 0.070314\n",
            "Train Epoch: 1 [1293750/60000 (37%)]\tLoss: 0.089051\n",
            "Train Epoch: 1 [1312500/60000 (37%)]\tLoss: 0.182374\n",
            "Train Epoch: 1 [1331250/60000 (38%)]\tLoss: 0.221655\n",
            "Train Epoch: 1 [1350000/60000 (38%)]\tLoss: 0.071175\n",
            "Train Epoch: 1 [1368750/60000 (39%)]\tLoss: 0.141559\n",
            "Train Epoch: 1 [1387500/60000 (39%)]\tLoss: 0.233555\n",
            "Train Epoch: 1 [1406250/60000 (40%)]\tLoss: 0.240934\n",
            "Train Epoch: 1 [1425000/60000 (41%)]\tLoss: 0.102202\n",
            "Train Epoch: 1 [1443750/60000 (41%)]\tLoss: 0.103164\n",
            "Train Epoch: 1 [1462500/60000 (42%)]\tLoss: 0.035982\n",
            "Train Epoch: 1 [1481250/60000 (42%)]\tLoss: 0.015108\n",
            "Train Epoch: 1 [1500000/60000 (43%)]\tLoss: 0.313333\n",
            "Train Epoch: 1 [1518750/60000 (43%)]\tLoss: 0.379051\n",
            "Train Epoch: 1 [1537500/60000 (44%)]\tLoss: 0.078576\n",
            "Train Epoch: 1 [1556250/60000 (44%)]\tLoss: 0.027308\n",
            "Train Epoch: 1 [1575000/60000 (45%)]\tLoss: 0.134700\n",
            "Train Epoch: 1 [1593750/60000 (45%)]\tLoss: 0.046455\n",
            "Train Epoch: 1 [1612500/60000 (46%)]\tLoss: 0.010250\n",
            "Train Epoch: 1 [1631250/60000 (46%)]\tLoss: 0.145000\n",
            "Train Epoch: 1 [1650000/60000 (47%)]\tLoss: 0.014384\n",
            "Train Epoch: 1 [1668750/60000 (47%)]\tLoss: 0.156841\n",
            "Train Epoch: 1 [1687500/60000 (48%)]\tLoss: 0.094955\n",
            "Train Epoch: 1 [1706250/60000 (49%)]\tLoss: 0.440771\n",
            "Train Epoch: 1 [1725000/60000 (49%)]\tLoss: 0.027345\n",
            "Train Epoch: 1 [1743750/60000 (50%)]\tLoss: 0.027497\n",
            "Train Epoch: 1 [1762500/60000 (50%)]\tLoss: 0.614930\n",
            "Train Epoch: 1 [1781250/60000 (51%)]\tLoss: 0.100981\n",
            "Train Epoch: 1 [1800000/60000 (51%)]\tLoss: 0.091339\n",
            "Train Epoch: 1 [1818750/60000 (52%)]\tLoss: 0.109984\n",
            "Train Epoch: 1 [1837500/60000 (52%)]\tLoss: 0.086247\n",
            "Train Epoch: 1 [1856250/60000 (53%)]\tLoss: 0.011481\n",
            "Train Epoch: 1 [1875000/60000 (53%)]\tLoss: 0.004046\n",
            "Train Epoch: 1 [1893750/60000 (54%)]\tLoss: 0.241525\n",
            "Train Epoch: 1 [1912500/60000 (54%)]\tLoss: 0.271081\n",
            "Train Epoch: 1 [1931250/60000 (55%)]\tLoss: 0.015282\n",
            "Train Epoch: 1 [1950000/60000 (55%)]\tLoss: 0.297040\n",
            "Train Epoch: 1 [1968750/60000 (56%)]\tLoss: 0.172558\n",
            "Train Epoch: 1 [1987500/60000 (57%)]\tLoss: 0.037709\n",
            "Train Epoch: 1 [2006250/60000 (57%)]\tLoss: 0.026609\n",
            "Train Epoch: 1 [2025000/60000 (58%)]\tLoss: 0.003170\n",
            "Train Epoch: 1 [2043750/60000 (58%)]\tLoss: 0.088325\n",
            "Train Epoch: 1 [2062500/60000 (59%)]\tLoss: 0.169207\n",
            "Train Epoch: 1 [2081250/60000 (59%)]\tLoss: 0.076714\n",
            "Train Epoch: 1 [2100000/60000 (60%)]\tLoss: 0.018446\n",
            "Train Epoch: 1 [2118750/60000 (60%)]\tLoss: 0.033162\n",
            "Train Epoch: 1 [2137500/60000 (61%)]\tLoss: 0.062365\n",
            "Train Epoch: 1 [2156250/60000 (61%)]\tLoss: 0.194921\n",
            "Train Epoch: 1 [2175000/60000 (62%)]\tLoss: 0.054334\n",
            "Train Epoch: 1 [2193750/60000 (62%)]\tLoss: 0.137992\n",
            "Train Epoch: 1 [2212500/60000 (63%)]\tLoss: 0.055806\n",
            "Train Epoch: 1 [2231250/60000 (63%)]\tLoss: 0.090045\n",
            "Train Epoch: 1 [2250000/60000 (64%)]\tLoss: 0.018487\n",
            "Train Epoch: 1 [2268750/60000 (65%)]\tLoss: 0.299618\n",
            "Train Epoch: 1 [2287500/60000 (65%)]\tLoss: 0.065663\n",
            "Train Epoch: 1 [2306250/60000 (66%)]\tLoss: 0.211076\n",
            "Train Epoch: 1 [2325000/60000 (66%)]\tLoss: 0.100653\n",
            "Train Epoch: 1 [2343750/60000 (67%)]\tLoss: 0.187461\n",
            "Train Epoch: 1 [2362500/60000 (67%)]\tLoss: 0.063256\n",
            "Train Epoch: 1 [2381250/60000 (68%)]\tLoss: 0.305055\n",
            "Train Epoch: 1 [2400000/60000 (68%)]\tLoss: 0.169850\n",
            "Train Epoch: 1 [2418750/60000 (69%)]\tLoss: 0.160682\n",
            "Train Epoch: 1 [2437500/60000 (69%)]\tLoss: 0.118077\n",
            "Train Epoch: 1 [2456250/60000 (70%)]\tLoss: 0.010001\n",
            "Train Epoch: 1 [2475000/60000 (70%)]\tLoss: 0.006138\n",
            "Train Epoch: 1 [2493750/60000 (71%)]\tLoss: 0.049280\n",
            "Train Epoch: 1 [2512500/60000 (71%)]\tLoss: 0.112402\n",
            "Train Epoch: 1 [2531250/60000 (72%)]\tLoss: 0.013122\n",
            "Train Epoch: 1 [2550000/60000 (73%)]\tLoss: 0.226453\n",
            "Train Epoch: 1 [2568750/60000 (73%)]\tLoss: 0.030415\n",
            "Train Epoch: 1 [2587500/60000 (74%)]\tLoss: 0.127177\n",
            "Train Epoch: 1 [2606250/60000 (74%)]\tLoss: 0.082164\n",
            "Train Epoch: 1 [2625000/60000 (75%)]\tLoss: 0.172343\n",
            "Train Epoch: 1 [2643750/60000 (75%)]\tLoss: 0.044551\n",
            "Train Epoch: 1 [2662500/60000 (76%)]\tLoss: 0.138321\n",
            "Train Epoch: 1 [2681250/60000 (76%)]\tLoss: 0.005995\n",
            "Train Epoch: 1 [2700000/60000 (77%)]\tLoss: 0.216514\n",
            "Train Epoch: 1 [2718750/60000 (77%)]\tLoss: 0.020241\n",
            "Train Epoch: 1 [2737500/60000 (78%)]\tLoss: 0.146494\n",
            "Train Epoch: 1 [2756250/60000 (78%)]\tLoss: 0.044552\n",
            "Train Epoch: 1 [2775000/60000 (79%)]\tLoss: 0.059862\n",
            "Train Epoch: 1 [2793750/60000 (79%)]\tLoss: 0.126248\n",
            "Train Epoch: 1 [2812500/60000 (80%)]\tLoss: 0.103952\n",
            "Train Epoch: 1 [2831250/60000 (81%)]\tLoss: 0.181175\n",
            "Train Epoch: 1 [2850000/60000 (81%)]\tLoss: 0.042951\n",
            "Train Epoch: 1 [2868750/60000 (82%)]\tLoss: 0.018962\n",
            "Train Epoch: 1 [2887500/60000 (82%)]\tLoss: 0.127000\n",
            "Train Epoch: 1 [2906250/60000 (83%)]\tLoss: 0.046980\n",
            "Train Epoch: 1 [2925000/60000 (83%)]\tLoss: 0.143692\n",
            "Train Epoch: 1 [2943750/60000 (84%)]\tLoss: 0.018079\n",
            "Train Epoch: 1 [2962500/60000 (84%)]\tLoss: 0.006633\n",
            "Train Epoch: 1 [2981250/60000 (85%)]\tLoss: 0.014559\n",
            "Train Epoch: 1 [3000000/60000 (85%)]\tLoss: 0.158235\n",
            "Train Epoch: 1 [3018750/60000 (86%)]\tLoss: 0.021185\n",
            "Train Epoch: 1 [3037500/60000 (86%)]\tLoss: 0.005785\n",
            "Train Epoch: 1 [3056250/60000 (87%)]\tLoss: 0.021199\n",
            "Train Epoch: 1 [3075000/60000 (87%)]\tLoss: 0.007973\n",
            "Train Epoch: 1 [3093750/60000 (88%)]\tLoss: 0.054388\n",
            "Train Epoch: 1 [3112500/60000 (89%)]\tLoss: 0.016420\n",
            "Train Epoch: 1 [3131250/60000 (89%)]\tLoss: 0.102080\n",
            "Train Epoch: 1 [3150000/60000 (90%)]\tLoss: 0.043372\n",
            "Train Epoch: 1 [3168750/60000 (90%)]\tLoss: 0.004146\n",
            "Train Epoch: 1 [3187500/60000 (91%)]\tLoss: 0.214500\n",
            "Train Epoch: 1 [3206250/60000 (91%)]\tLoss: 0.005493\n",
            "Train Epoch: 1 [3225000/60000 (92%)]\tLoss: 0.009800\n",
            "Train Epoch: 1 [3243750/60000 (92%)]\tLoss: 0.072543\n",
            "Train Epoch: 1 [3262500/60000 (93%)]\tLoss: 0.038846\n",
            "Train Epoch: 1 [3281250/60000 (93%)]\tLoss: 0.087638\n",
            "Train Epoch: 1 [3300000/60000 (94%)]\tLoss: 0.038976\n",
            "Train Epoch: 1 [3318750/60000 (94%)]\tLoss: 0.024199\n",
            "Train Epoch: 1 [3337500/60000 (95%)]\tLoss: 0.134617\n",
            "Train Epoch: 1 [3356250/60000 (95%)]\tLoss: 0.127466\n",
            "Train Epoch: 1 [3375000/60000 (96%)]\tLoss: 0.004923\n",
            "Train Epoch: 1 [3393750/60000 (97%)]\tLoss: 0.130625\n",
            "Train Epoch: 1 [3412500/60000 (97%)]\tLoss: 0.050939\n",
            "Train Epoch: 1 [3431250/60000 (98%)]\tLoss: 0.013347\n",
            "Train Epoch: 1 [3450000/60000 (98%)]\tLoss: 0.049535\n",
            "Train Epoch: 1 [3468750/60000 (99%)]\tLoss: 0.003558\n",
            "Train Epoch: 1 [3487500/60000 (99%)]\tLoss: 0.030902\n",
            "Train Epoch: 1 [3506250/60000 (100%)]\tLoss: 0.025830\n",
            "\n",
            "Test set: Average loss: 0.0469, Accuracy: 9854/10000 (99%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.010855\n",
            "Train Epoch: 2 [18750/60000 (1%)]\tLoss: 0.041931\n",
            "Train Epoch: 2 [37500/60000 (1%)]\tLoss: 0.045512\n",
            "Train Epoch: 2 [56250/60000 (2%)]\tLoss: 0.051548\n",
            "Train Epoch: 2 [75000/60000 (2%)]\tLoss: 0.015521\n",
            "Train Epoch: 2 [93750/60000 (3%)]\tLoss: 0.218134\n",
            "Train Epoch: 2 [112500/60000 (3%)]\tLoss: 0.360810\n",
            "Train Epoch: 2 [131250/60000 (4%)]\tLoss: 0.007315\n",
            "Train Epoch: 2 [150000/60000 (4%)]\tLoss: 0.103328\n",
            "Train Epoch: 2 [168750/60000 (5%)]\tLoss: 0.188341\n",
            "Train Epoch: 2 [187500/60000 (5%)]\tLoss: 0.029456\n",
            "Train Epoch: 2 [206250/60000 (6%)]\tLoss: 0.002012\n",
            "Train Epoch: 2 [225000/60000 (6%)]\tLoss: 0.003388\n",
            "Train Epoch: 2 [243750/60000 (7%)]\tLoss: 0.021964\n",
            "Train Epoch: 2 [262500/60000 (7%)]\tLoss: 0.017218\n",
            "Train Epoch: 2 [281250/60000 (8%)]\tLoss: 0.092981\n",
            "Train Epoch: 2 [300000/60000 (9%)]\tLoss: 0.134518\n",
            "Train Epoch: 2 [318750/60000 (9%)]\tLoss: 0.098668\n",
            "Train Epoch: 2 [337500/60000 (10%)]\tLoss: 0.063112\n",
            "Train Epoch: 2 [356250/60000 (10%)]\tLoss: 0.008568\n",
            "Train Epoch: 2 [375000/60000 (11%)]\tLoss: 0.007091\n",
            "Train Epoch: 2 [393750/60000 (11%)]\tLoss: 0.020968\n",
            "Train Epoch: 2 [412500/60000 (12%)]\tLoss: 0.050781\n",
            "Train Epoch: 2 [431250/60000 (12%)]\tLoss: 0.017521\n",
            "Train Epoch: 2 [450000/60000 (13%)]\tLoss: 0.003867\n",
            "Train Epoch: 2 [468750/60000 (13%)]\tLoss: 0.157694\n",
            "Train Epoch: 2 [487500/60000 (14%)]\tLoss: 0.112393\n",
            "Train Epoch: 2 [506250/60000 (14%)]\tLoss: 0.070411\n",
            "Train Epoch: 2 [525000/60000 (15%)]\tLoss: 0.106890\n",
            "Train Epoch: 2 [543750/60000 (15%)]\tLoss: 0.110064\n",
            "Train Epoch: 2 [562500/60000 (16%)]\tLoss: 0.014808\n",
            "Train Epoch: 2 [581250/60000 (17%)]\tLoss: 0.196460\n",
            "Train Epoch: 2 [600000/60000 (17%)]\tLoss: 0.013155\n",
            "Train Epoch: 2 [618750/60000 (18%)]\tLoss: 0.021942\n",
            "Train Epoch: 2 [637500/60000 (18%)]\tLoss: 0.106792\n",
            "Train Epoch: 2 [656250/60000 (19%)]\tLoss: 0.018621\n",
            "Train Epoch: 2 [675000/60000 (19%)]\tLoss: 0.027234\n",
            "Train Epoch: 2 [693750/60000 (20%)]\tLoss: 0.023237\n",
            "Train Epoch: 2 [712500/60000 (20%)]\tLoss: 0.005309\n",
            "Train Epoch: 2 [731250/60000 (21%)]\tLoss: 0.233014\n",
            "Train Epoch: 2 [750000/60000 (21%)]\tLoss: 0.025116\n",
            "Train Epoch: 2 [768750/60000 (22%)]\tLoss: 0.018269\n",
            "Train Epoch: 2 [787500/60000 (22%)]\tLoss: 0.001990\n",
            "Train Epoch: 2 [806250/60000 (23%)]\tLoss: 0.026253\n",
            "Train Epoch: 2 [825000/60000 (23%)]\tLoss: 0.014393\n",
            "Train Epoch: 2 [843750/60000 (24%)]\tLoss: 0.231034\n",
            "Train Epoch: 2 [862500/60000 (25%)]\tLoss: 0.005752\n",
            "Train Epoch: 2 [881250/60000 (25%)]\tLoss: 0.003620\n",
            "Train Epoch: 2 [900000/60000 (26%)]\tLoss: 0.009417\n",
            "Train Epoch: 2 [918750/60000 (26%)]\tLoss: 0.049866\n",
            "Train Epoch: 2 [937500/60000 (27%)]\tLoss: 0.141923\n",
            "Train Epoch: 2 [956250/60000 (27%)]\tLoss: 0.266318\n",
            "Train Epoch: 2 [975000/60000 (28%)]\tLoss: 0.088984\n",
            "Train Epoch: 2 [993750/60000 (28%)]\tLoss: 0.090733\n",
            "Train Epoch: 2 [1012500/60000 (29%)]\tLoss: 0.006526\n",
            "Train Epoch: 2 [1031250/60000 (29%)]\tLoss: 0.017488\n",
            "Train Epoch: 2 [1050000/60000 (30%)]\tLoss: 0.065503\n",
            "Train Epoch: 2 [1068750/60000 (30%)]\tLoss: 0.085752\n",
            "Train Epoch: 2 [1087500/60000 (31%)]\tLoss: 0.022595\n",
            "Train Epoch: 2 [1106250/60000 (31%)]\tLoss: 0.024656\n",
            "Train Epoch: 2 [1125000/60000 (32%)]\tLoss: 0.034364\n",
            "Train Epoch: 2 [1143750/60000 (33%)]\tLoss: 0.060092\n",
            "Train Epoch: 2 [1162500/60000 (33%)]\tLoss: 0.002135\n",
            "Train Epoch: 2 [1181250/60000 (34%)]\tLoss: 0.006574\n",
            "Train Epoch: 2 [1200000/60000 (34%)]\tLoss: 0.001012\n",
            "Train Epoch: 2 [1218750/60000 (35%)]\tLoss: 0.024499\n",
            "Train Epoch: 2 [1237500/60000 (35%)]\tLoss: 0.114917\n",
            "Train Epoch: 2 [1256250/60000 (36%)]\tLoss: 0.031535\n",
            "Train Epoch: 2 [1275000/60000 (36%)]\tLoss: 0.063283\n",
            "Train Epoch: 2 [1293750/60000 (37%)]\tLoss: 0.007706\n",
            "Train Epoch: 2 [1312500/60000 (37%)]\tLoss: 0.176220\n",
            "Train Epoch: 2 [1331250/60000 (38%)]\tLoss: 0.139552\n",
            "Train Epoch: 2 [1350000/60000 (38%)]\tLoss: 0.024526\n",
            "Train Epoch: 2 [1368750/60000 (39%)]\tLoss: 0.015094\n",
            "Train Epoch: 2 [1387500/60000 (39%)]\tLoss: 0.047365\n",
            "Train Epoch: 2 [1406250/60000 (40%)]\tLoss: 0.008938\n",
            "Train Epoch: 2 [1425000/60000 (41%)]\tLoss: 0.538301\n",
            "Train Epoch: 2 [1443750/60000 (41%)]\tLoss: 0.007102\n",
            "Train Epoch: 2 [1462500/60000 (42%)]\tLoss: 0.013807\n",
            "Train Epoch: 2 [1481250/60000 (42%)]\tLoss: 0.008198\n",
            "Train Epoch: 2 [1500000/60000 (43%)]\tLoss: 0.024592\n",
            "Train Epoch: 2 [1518750/60000 (43%)]\tLoss: 0.096521\n",
            "Train Epoch: 2 [1537500/60000 (44%)]\tLoss: 0.040757\n",
            "Train Epoch: 2 [1556250/60000 (44%)]\tLoss: 0.002261\n",
            "Train Epoch: 2 [1575000/60000 (45%)]\tLoss: 0.015169\n",
            "Train Epoch: 2 [1593750/60000 (45%)]\tLoss: 0.003878\n",
            "Train Epoch: 2 [1612500/60000 (46%)]\tLoss: 0.049256\n",
            "Train Epoch: 2 [1631250/60000 (46%)]\tLoss: 0.014313\n",
            "Train Epoch: 2 [1650000/60000 (47%)]\tLoss: 0.002527\n",
            "Train Epoch: 2 [1668750/60000 (47%)]\tLoss: 0.102115\n",
            "Train Epoch: 2 [1687500/60000 (48%)]\tLoss: 0.194585\n",
            "Train Epoch: 2 [1706250/60000 (49%)]\tLoss: 0.121427\n",
            "Train Epoch: 2 [1725000/60000 (49%)]\tLoss: 0.002761\n",
            "Train Epoch: 2 [1743750/60000 (50%)]\tLoss: 0.028475\n",
            "Train Epoch: 2 [1762500/60000 (50%)]\tLoss: 0.097916\n",
            "Train Epoch: 2 [1781250/60000 (51%)]\tLoss: 0.047670\n",
            "Train Epoch: 2 [1800000/60000 (51%)]\tLoss: 0.034870\n",
            "Train Epoch: 2 [1818750/60000 (52%)]\tLoss: 0.013956\n",
            "Train Epoch: 2 [1837500/60000 (52%)]\tLoss: 0.013987\n",
            "Train Epoch: 2 [1856250/60000 (53%)]\tLoss: 0.045418\n",
            "Train Epoch: 2 [1875000/60000 (53%)]\tLoss: 0.083633\n",
            "Train Epoch: 2 [1893750/60000 (54%)]\tLoss: 0.022954\n",
            "Train Epoch: 2 [1912500/60000 (54%)]\tLoss: 0.027691\n",
            "Train Epoch: 2 [1931250/60000 (55%)]\tLoss: 0.025002\n",
            "Train Epoch: 2 [1950000/60000 (55%)]\tLoss: 0.040624\n",
            "Train Epoch: 2 [1968750/60000 (56%)]\tLoss: 0.044654\n",
            "Train Epoch: 2 [1987500/60000 (57%)]\tLoss: 0.081245\n",
            "Train Epoch: 2 [2006250/60000 (57%)]\tLoss: 0.174525\n",
            "Train Epoch: 2 [2025000/60000 (58%)]\tLoss: 0.295458\n",
            "Train Epoch: 2 [2043750/60000 (58%)]\tLoss: 0.009157\n",
            "Train Epoch: 2 [2062500/60000 (59%)]\tLoss: 0.126573\n",
            "Train Epoch: 2 [2081250/60000 (59%)]\tLoss: 0.005754\n",
            "Train Epoch: 2 [2100000/60000 (60%)]\tLoss: 0.003640\n",
            "Train Epoch: 2 [2118750/60000 (60%)]\tLoss: 0.004727\n",
            "Train Epoch: 2 [2137500/60000 (61%)]\tLoss: 0.024353\n",
            "Train Epoch: 2 [2156250/60000 (61%)]\tLoss: 0.011709\n",
            "Train Epoch: 2 [2175000/60000 (62%)]\tLoss: 0.011798\n",
            "Train Epoch: 2 [2193750/60000 (62%)]\tLoss: 0.019631\n",
            "Train Epoch: 2 [2212500/60000 (63%)]\tLoss: 0.284811\n",
            "Train Epoch: 2 [2231250/60000 (63%)]\tLoss: 0.159131\n",
            "Train Epoch: 2 [2250000/60000 (64%)]\tLoss: 0.030853\n",
            "Train Epoch: 2 [2268750/60000 (65%)]\tLoss: 0.120129\n",
            "Train Epoch: 2 [2287500/60000 (65%)]\tLoss: 0.004867\n",
            "Train Epoch: 2 [2306250/60000 (66%)]\tLoss: 0.077086\n",
            "Train Epoch: 2 [2325000/60000 (66%)]\tLoss: 0.174588\n",
            "Train Epoch: 2 [2343750/60000 (67%)]\tLoss: 0.013499\n",
            "Train Epoch: 2 [2362500/60000 (67%)]\tLoss: 0.029880\n",
            "Train Epoch: 2 [2381250/60000 (68%)]\tLoss: 0.066804\n",
            "Train Epoch: 2 [2400000/60000 (68%)]\tLoss: 0.027398\n",
            "Train Epoch: 2 [2418750/60000 (69%)]\tLoss: 0.041973\n",
            "Train Epoch: 2 [2437500/60000 (69%)]\tLoss: 0.014386\n",
            "Train Epoch: 2 [2456250/60000 (70%)]\tLoss: 0.000745\n",
            "Train Epoch: 2 [2475000/60000 (70%)]\tLoss: 0.001556\n",
            "Train Epoch: 2 [2493750/60000 (71%)]\tLoss: 0.166840\n",
            "Train Epoch: 2 [2512500/60000 (71%)]\tLoss: 0.034720\n",
            "Train Epoch: 2 [2531250/60000 (72%)]\tLoss: 0.002711\n",
            "Train Epoch: 2 [2550000/60000 (73%)]\tLoss: 0.061102\n",
            "Train Epoch: 2 [2568750/60000 (73%)]\tLoss: 0.075874\n",
            "Train Epoch: 2 [2587500/60000 (74%)]\tLoss: 0.044077\n",
            "Train Epoch: 2 [2606250/60000 (74%)]\tLoss: 0.001837\n",
            "Train Epoch: 2 [2625000/60000 (75%)]\tLoss: 0.390540\n",
            "Train Epoch: 2 [2643750/60000 (75%)]\tLoss: 0.026890\n",
            "Train Epoch: 2 [2662500/60000 (76%)]\tLoss: 0.572088\n",
            "Train Epoch: 2 [2681250/60000 (76%)]\tLoss: 0.013560\n",
            "Train Epoch: 2 [2700000/60000 (77%)]\tLoss: 0.005997\n",
            "Train Epoch: 2 [2718750/60000 (77%)]\tLoss: 0.027759\n",
            "Train Epoch: 2 [2737500/60000 (78%)]\tLoss: 0.002944\n",
            "Train Epoch: 2 [2756250/60000 (78%)]\tLoss: 0.012996\n",
            "Train Epoch: 2 [2775000/60000 (79%)]\tLoss: 0.244365\n",
            "Train Epoch: 2 [2793750/60000 (79%)]\tLoss: 0.087220\n",
            "Train Epoch: 2 [2812500/60000 (80%)]\tLoss: 0.030822\n",
            "Train Epoch: 2 [2831250/60000 (81%)]\tLoss: 0.094380\n",
            "Train Epoch: 2 [2850000/60000 (81%)]\tLoss: 0.003843\n",
            "Train Epoch: 2 [2868750/60000 (82%)]\tLoss: 0.015409\n",
            "Train Epoch: 2 [2887500/60000 (82%)]\tLoss: 0.042154\n",
            "Train Epoch: 2 [2906250/60000 (83%)]\tLoss: 0.006017\n",
            "Train Epoch: 2 [2925000/60000 (83%)]\tLoss: 0.393340\n",
            "Train Epoch: 2 [2943750/60000 (84%)]\tLoss: 0.098811\n",
            "Train Epoch: 2 [2962500/60000 (84%)]\tLoss: 0.108196\n",
            "Train Epoch: 2 [2981250/60000 (85%)]\tLoss: 0.054715\n",
            "Train Epoch: 2 [3000000/60000 (85%)]\tLoss: 0.093734\n",
            "Train Epoch: 2 [3018750/60000 (86%)]\tLoss: 0.176219\n",
            "Train Epoch: 2 [3037500/60000 (86%)]\tLoss: 0.078910\n",
            "Train Epoch: 2 [3056250/60000 (87%)]\tLoss: 0.016085\n",
            "Train Epoch: 2 [3075000/60000 (87%)]\tLoss: 0.004420\n",
            "Train Epoch: 2 [3093750/60000 (88%)]\tLoss: 0.010175\n",
            "Train Epoch: 2 [3112500/60000 (89%)]\tLoss: 0.018805\n",
            "Train Epoch: 2 [3131250/60000 (89%)]\tLoss: 0.006323\n",
            "Train Epoch: 2 [3150000/60000 (90%)]\tLoss: 0.003779\n",
            "Train Epoch: 2 [3168750/60000 (90%)]\tLoss: 0.129702\n",
            "Train Epoch: 2 [3187500/60000 (91%)]\tLoss: 0.003618\n",
            "Train Epoch: 2 [3206250/60000 (91%)]\tLoss: 0.183232\n",
            "Train Epoch: 2 [3225000/60000 (92%)]\tLoss: 0.168080\n",
            "Train Epoch: 2 [3243750/60000 (92%)]\tLoss: 0.034751\n",
            "Train Epoch: 2 [3262500/60000 (93%)]\tLoss: 0.006751\n",
            "Train Epoch: 2 [3281250/60000 (93%)]\tLoss: 0.064957\n",
            "Train Epoch: 2 [3300000/60000 (94%)]\tLoss: 0.011202\n",
            "Train Epoch: 2 [3318750/60000 (94%)]\tLoss: 0.000753\n",
            "Train Epoch: 2 [3337500/60000 (95%)]\tLoss: 0.072605\n",
            "Train Epoch: 2 [3356250/60000 (95%)]\tLoss: 0.084270\n",
            "Train Epoch: 2 [3375000/60000 (96%)]\tLoss: 0.285276\n",
            "Train Epoch: 2 [3393750/60000 (97%)]\tLoss: 0.084342\n",
            "Train Epoch: 2 [3412500/60000 (97%)]\tLoss: 0.055352\n",
            "Train Epoch: 2 [3431250/60000 (98%)]\tLoss: 0.003126\n",
            "Train Epoch: 2 [3450000/60000 (98%)]\tLoss: 0.007343\n",
            "Train Epoch: 2 [3468750/60000 (99%)]\tLoss: 0.001006\n",
            "Train Epoch: 2 [3487500/60000 (99%)]\tLoss: 0.002261\n",
            "Train Epoch: 2 [3506250/60000 (100%)]\tLoss: 0.121602\n",
            "\n",
            "Test set: Average loss: 0.0371, Accuracy: 9877/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 훈련 후 테스트셋을 통해 성능도 검증했으니 샘플 이미지에서 추론이 맞는지 확인\n",
        "test_samples = enumerate(test_dataloader)\n",
        "b_i, (sample_data, sample_targets) = next(test_samples)\n",
        "\n",
        "# 확률이 가장 높은 클래스(max)를 선택. model(sample_data)[1]은 숫자 분류 결과 배열.\n",
        "print(f\"model prediction : {model(sample_data).data.max(1)[1][0]}\")\n",
        "print(f\"Ground truth : {sample_targets[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfkldWN9c-M4",
        "outputId": "d2dc30c4-0ac7-4c44-e1b3-b7dec21da3aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model prediction : 0\n",
            "Ground truth : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN과 LSTM 결합하기"
      ],
      "metadata": {
        "id": "fqcar5Xwg4Ur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 챕터에서는 CNN과 LSTM을 결합한 하이브리드 모델을 이용한 이미지 캡션(이미지 설명) 실습 수행.\n",
        "\n",
        "CNN은 이미지를 가져와 고차원 특징이나 임베딩을 출력하는 인코더로 사용.\n",
        "LSTM은 CNN의 마지막 은닉층을 입력값으로 받아 텍스트를 생성하는 디코더.\n",
        "\n",
        "LSTM은 t=0에서 CNN의 임베딩(벡터)을 입력으로 가져옴."
      ],
      "metadata": {
        "id": "mbczLVpXhLyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## linux\n",
        "!apt-get install wget\n",
        "\n",
        "## create a data directory\n",
        "!mkdir data_dir\n",
        "\n",
        "## download images and annotations to the data directory\n",
        "!wget http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip -P ./data_dir/\n",
        "!wget http://images.cocodataset.org/zips/train2014.zip -P ./data_dir/\n",
        "!wget http://images.cocodataset.org/zips/val2014.zip -P ./data_dir/\n",
        "\n",
        "## extract zipped images and annotations and remove the zip files\n",
        "!unzip ./data_dir/captions_train-val2014.zip -d ./data_dir/\n",
        "!rm ./data_dir/captions_train-val2014.zip\n",
        "!unzip ./data_dir/train2014.zip -d ./data_dir/\n",
        "!rm ./data_dir/train2014.zip\n",
        "!unzip ./data_dir/val2014.zip -d ./data_dir/\n",
        "!rm ./data_dir/val2014.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "k_Md4By8n_FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# 자연어 툴킷, 사전을 구축할 때 사용\n",
        "import nltk\n",
        "# 파이썬 객체를 직렬화/역직렬화 하는 라이브러리\n",
        "import pickle\n",
        "import numpy as np\n",
        "# 이미지 파일을 다룰 수 있게 해주는 라이브러리\n",
        "from PIL import Image\n",
        "# 데이터의 빈도수를 셀 수 있는 도구\n",
        "from collections import Counter\n",
        "# COCO데이터셋을 가지고 작업할 때 유용\n",
        "from pycocotools.coco import COCO\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "# 다양한 길이의 문장에 패딩을 적용해 고장된 길이의 문장으로 변환할 때 사용하는 라이브러리\n",
        "from torch.nn.utils.rnn import pack_padded_sequence"
      ],
      "metadata": {
        "id": "nXV1mUHop6zx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# punkt 토크나이저 모델 다운. 텍스트를 단어로 토큰화\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDrstlB-ulb3",
        "outputId": "5f4a0c85-7623-4745-860c-7263842834a7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab(object):\n",
        "    \"\"\"\n",
        "    간단한 사전 객체\n",
        "\n",
        "    vocabulary(사전) : 단어집합 OR 자연어 처리 시스템이 인식,처리 할 수 있는 단어,토큰 집합. 즉,\n",
        "                       단어를 숫자로 매핑하여 그걸 저장하고 있음.\n",
        "\n",
        "    wrapper : vocabulary을 관리하는 클래스/함수.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # word에서 index로 매핑된 값을 저장하는 딕셔너리. ex) 'hello':1 이런 걸 저장\n",
        "        self.w2i = {}\n",
        "        # index에서 단어로의 매핑을 저장하는 딕셔너리. ex) 1:'hello' 이런 걸 저장\n",
        "        self.i2w = {}\n",
        "        # 새로운 단어가 추가될 때마다 값이 증가\n",
        "        self.index = 0\n",
        "\n",
        "    def __call__(self, token):\n",
        "        # 만약 아무 단어도 매핑이 안 되었으면\n",
        "        if not token in self.w2i:\n",
        "            # unknown toekn의 인덱스를 반환\n",
        "            return self.w2i['<unk>']\n",
        "        # 만약 토큰이 존재하면(즉 단어가 이미 매핑 되어 있었다면), 해당 단어에 대응하는 인덱스 출력\n",
        "        return self.w2i[token]\n",
        "\n",
        "    # 단어집합의 크기(단어 개수)를 반환\n",
        "    def __len__(self):\n",
        "        return len(self.w2i)\n",
        "\n",
        "    # 새로운 단어를 사전에 추가\n",
        "    def add_token(self, token):\n",
        "        # 단어가 매핑되어 저장되어 있지 않았다면\n",
        "        if not token in self.w2i:\n",
        "            # 해당 단어(토큰)가 key가 되고, index를 value로 설정하여 딕셔너리에 저장\n",
        "            self.w2i[token] = self.index\n",
        "            # 얘도 마찬가지\n",
        "            self.i2w[self.index] = token\n",
        "            # 단어 새로 추가됐으니까 또다른 새로운 단어의 value로 쓰일 인덱스 값 만들어주기\n",
        "            self.index += 1\n",
        "\n",
        "\n",
        "# 텍스트 토큰을 숫자 토큰으로 전환할 수 있는 사전을 구축하는 함수\n",
        "def build_vocabulary(json, threshold):\n",
        "  # 사전 wrapper를 구축(사전(단어집합)을 관리하는 함수를 구축)\n",
        "  coco = COCO(json)\n",
        "  counter = Counter()\n",
        "  # COCO객체인 coco의 ann(annotation)의 키값들을 ids에 담기(ann이 dict로 추정)\n",
        "  ids = coco.anns.keys()\n",
        "  for i, id in enumerate(ids):\n",
        "    # 캡션(이미지 설명 문장)을 각 이미지로부터 가져오기\n",
        "    caption = str(coco.anns[id]['caption'])\n",
        "    # tokenize 모듈에 포함된 word_tokenize 함수로 문자열을 단어 단위로 분리(공백, 구두점 등도 알아서 처리)\n",
        "    tokens = nltk.tokenize.word_tokenize(caption.lower()) # caption을 소문자로 변환, 리스트가 반환됨\n",
        "    # iterable을 인자로 받아 그 안의 요소들의 빈도를 Counter 객체에 추가(OR 기존을 업데이트)\n",
        "    counter.update(tokens)\n",
        "    if (i+1) % 1000 == 0:\n",
        "      print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(ids)))\n",
        "  # 빈도수가 threshold이상인 토큰들만 분류하여 리스트에 담기\n",
        "  tokens = [token for token, cnt in counter.items() if cnt>=threshold]\n",
        "  # vocab wrapper를 만들고 특수한 토큰을 수동으로 추가\n",
        "  vocab = Vocab()\n",
        "  vocab.add_token(\"<pad>\")\n",
        "  vocab.add_token(\"<start>\")\n",
        "  vocab.add_token(\"<end>\")\n",
        "  vocab.add_token(\"<unk>\")\n",
        "  # w2i, i2w에 tokenize 후 넣어주기(인덱스는 add 되면서 자동으로 각 token에 할당된다)\n",
        "  for _, token in enumerate(tokens):\n",
        "    vocab.add_token(token)\n",
        "  # tokenize 후 indexing도 다 된(즉, 매핑이 끝난) 사전을 반환\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "XfddMWVDuu9l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4번 이상 나온 단어들만 골라내어 하나의 집합으로 만듦\n",
        "vocab = build_vocabulary(json='data_dir/annotations/captions_train2014.json', threshold=4) # 책 따라하니까 설명이 생략된 게 좀 있어 보이는데 COCO 웹사이트 가서 captions 직접 다운함\n",
        "vocab_path = './data_dir/vocabulary.pkl'\n",
        "with open(vocab_path, 'wb') as f: # ./data_dir vocabulary.pkl 파일을 만들어 바이너리 쓰기 모드로 열어서\n",
        "    pickle.dump(vocab, f) # 사전객체(vocab)를 직렬화하여 파일에 저장->사전 객체를 로컬에 저장하여 나중에 사전 구축 함수를 재구성하는 수고를 덜 수 있음.\n",
        "print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
        "print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ij3FTWc627Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN모델의 첫 번째 계층에 입력으로 제공될 수 있도록 전체 이미지를 고정된 모양으로 변형\n",
        "\n",
        "# 개별 이미지의 사이즈를 정해진 shape으로 바꾸기\n",
        "def reshape_image(image, shape):\n",
        "  return image.resize(shape, Image.ANTIALIAS)\n",
        "\n",
        "# 이미지 데이터들을 리사이징\n",
        "def reshape_images(image_path, output_path, shape):\n",
        "  \"\"\"\n",
        "  image_path : shape을 resize할 이미지들이 모여있는 위치\n",
        "\n",
        "  output_path : 변형을 마친 이미지들이 저장될 위치\n",
        "  \"\"\"\n",
        "\n",
        "  # output_path라는 폴더가 없다면\n",
        "  if not os.path.exists(output_path):\n",
        "    # 하나 만들어라\n",
        "    os.makedirs(output_path)\n",
        "  # image_path 디렉토리에 있는 파일과 폴더를 가져옴\n",
        "  images = os.listdir(image_path)\n",
        "  num_img = len(images)\n",
        "  # images 변수에 저장된 개별 image들을\n",
        "  for i, img in enumerate(images):\n",
        "    # image_path에 있는 img 파일들을 open하고(이미지 파일은 바이너리니까 b) f에 데이터를 담고\n",
        "    with open(os.path.join(image_path, img), 'r+b') as f:\n",
        "      # PIL 라이브러리의 Image메서드를 통해 해당 f에 담긴 이미지 파일 객체를 열어 그걸 변수 image에 담고\n",
        "      with Image.open(f) as image:\n",
        "        # 모든 이미지들이 사이즈가 제각각이므로 이미지를 고정된 shape으로 다 reshaping\n",
        "        image = reshape_image(image, shape)\n",
        "        # Pillow의 Image 객체의 save메서드를 이용해 output_path 경로에 img라는 이름의 이미지 파일 저장\n",
        "        image.save(os.path.join(output_path, img), image.format) # 그리고 이미지 파일 객체의 파일 형식 해당 이미지 파일의 현재 형식으로 설정\n",
        "      if (i+1) % 100 == 0:\n",
        "        print(\"[{}/{}] Resized the images and saved into {}.\".format(i+1, num_img, output_path))"
      ],
      "metadata": {
        "id": "xX3DVjvd8mjJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = './data_dir/train2014/'\n",
        "output_path = './data_dir/resized_images/'\n",
        "# 모든 이미지를 256X256의 size(=shape)로 만든다\n",
        "image_shape = [256, 256]\n",
        "reshape_images(image_path, output_path, image_shape)"
      ],
      "metadata": {
        "id": "-qZ92i19kIic",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "지금까지 이미지와 해당 이미지의 캡션 데이터를 받아 전처리를 했다(reshaping, tokenizing) 이제 이 데이터를 파이토치 데이터셋으로 변환"
      ],
      "metadata": {
        "id": "Jrhry7xymqBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCocoDataset(data.Dataset):\n",
        "  def __init__(self, data_path, coco_json_path, vocabulary, transform=None):\n",
        "    \"\"\"\n",
        "    data_path : 이미지 데이터 경로\n",
        "    coco_json_path : coco annotation 파일 경로\n",
        "    vocabulary : 우리가 만든 사전객체(vocabulary.pkl)\n",
        "    transform : 이미지 transformer(이미 resizing했으니까 노필요)\n",
        "    \"\"\"\n",
        "    self.root = data_path\n",
        "    self.coco_data = COCO(coco_json_path) # 주어진 coco_json_path(annotation파일의 경로)로부터 JSON파일을 읽어 데이터 제공\n",
        "    self.indices = list(self.coco_data.anns.keys())\n",
        "    self.vocabulary = vocabulary\n",
        "    self.transform = transform\n",
        "\n",
        "  # image와 caption의 한 페어를 반환하는 메서드\n",
        "  def __getitem__(self, idx):\n",
        "    coco_data = self.coco_data\n",
        "    vocabulary = self.vocabulary\n",
        "    annotation_id = self.indices[idx]\n",
        "    caption = coco_data.anns[annotation_id]['caption']\n",
        "    image_id = coco_data.anns[annotation_id]['image_id']\n",
        "    image_path = coco_data.loadImgs(image_id)[0]['file_name']\n",
        "    image = Image.open(os.path.join(self.root, image_path)).convert('RGB')\n",
        "    # 만약 transform을 위한 모델을 따로 넣었다면\n",
        "    if self.transform is not None:\n",
        "      # 그 변환 모델로 이미지 변형\n",
        "      image = self.transform(image)\n",
        "    # caption들을 tokenize(단어->숫자화)\n",
        "    word_tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
        "    caption = []\n",
        "    caption.append(vocabulary('<start>'))\n",
        "    # 토큰화된 단어들을 하나의 리스트에 모아서 한 번에 caption리스트에 넣기\n",
        "    caption.extend([vocabulary(token) for token in word_tokens])\n",
        "    caption.append(vocabulary('<end>'))\n",
        "    # true값들을 모은 리스트를 파이토치 텐서로 바꾸는 것.\n",
        "    ground_truth = torch.Tensor(caption)\n",
        "    return image, ground_truth\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.indices)\n",
        "\n",
        "\n",
        "# X, y 형태로 데이터의 미니배치를 반환하는 함수\n",
        "def collate_function(data_batch):\n",
        "  \"\"\"\n",
        "  (image, caption)형태의 튜플들을 담은 리스트로부터 미니배치 텐서를 만들어주는 함수\n",
        "\n",
        "  기본적으로 collate_fn가 제공되지만, 캡션(자연어)에 대한 패딩과 병합이 되지 않으므로 custom으로 만드는 것\n",
        "\n",
        "  반환하는 것은 (batch_size, 3, 256, 256) shape의 images 텐서, (batch_size, padded_length) shape의 targets 텐서,\n",
        "  padding 된 캡션의 길이\n",
        "  \"\"\"\n",
        "\n",
        "  # (image, caption)이 들어있는 data_batch 리스트를 내림차순으로 정렬하는데 기준(key)은 각각의 (image, caption)의 caption길이\n",
        "  data_batch.sort(key=lambda d: len(d[1]), reverse=True)\n",
        "  imgs, caps = zip(*data_batch) # 각 (image, caption) 쌍을 (img1, img2...), (cap1, cap2...)의 형태로 각 변수에 할당\n",
        "  # imgs에는 batch_size만큼의 img들이 들어 있고, 각 img들은 (3,256,256)의 shape의 matrix인데\n",
        "  imgs = torch.stack(imgs, 0) # stack 통해서 (batch_size,3,256,256)의 shape으로 변함\n",
        "\n",
        "  # 캡션도 원래는 1차원이지만, batch_size를 고려하여 (batch_size, 캡션 길이)의 2차원으로 변경\n",
        "  cap_lens = [len(cap) for cap in caps]\n",
        "  # 모든 캡션을 담을 수 있는 빈 텐서 생성\n",
        "  tgts = torch.zeros(len(caps), max(cap_lens)).long() # len(caps): 배치 내의 캡션 개수, 즉 배치 크기 max(cap_lens): 배치에 가장 긴 캡션. 얘를 기준으로 모든 캡션을 동일 길이로 맞추기 위한 텐서의 두 번째 차원\n",
        "  for i, cap in enumerate(caps):\n",
        "    end = cap_lens[i] # 각 캡션의 실제 길이(단어 수)를 가져와서\n",
        "    tgts[i, :end] = cap[:end] # i번째 캡션에 대해 텐서를 채워 넣는다\n",
        "  return imgs, tgts, cap_lens\n",
        "\n",
        "\n",
        "# coco dataset을 위한 torch.utils.data.DataLoader을 반환하는 함수. 훈련모드에서 데이터의 미니 배치를 가져오는데 유용\n",
        "def get_loader(data_path, coco_json_path, vocabulary, transform, batch_size, shuffle, num_workers):\n",
        "  \"\"\"\n",
        "  반환하는 것은 (batch_size, 3, 256, 256) shape의 images 텐서, (batch_size, padded_length) shape의 targets 텐서,\n",
        "  padding 된 캡션의 길이\n",
        "  \"\"\"\n",
        "  coco_dataset = CustomCocoDataset(data_path=data_path, coco_json_path = coco_json_path, vocabulary=vocabulary, transform=transform)\n",
        "\n",
        "  # 한 번의 주기(iteration)에서 (images, captions, lengths)를 반환\n",
        "  custom_data_loader = torch.utils.data.DataLoader(dataset=coco_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=shuffle,\n",
        "                                          num_workers=num_workers,\n",
        "                                          collate_fn=collate_function)\n",
        "  return custom_data_loader"
      ],
      "metadata": {
        "id": "kInsSHjhmI9z"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "  def __init__(self, embedding_size):\n",
        "    # 사전학습 된 ResNet-152를 불러온다 CNN의 출력 결과를 생성하는 fc layer를 대체\n",
        "    super(CNNModel, self).__init__()\n",
        "    resnet = models.resnet152(pretrained=True)\n",
        "    module_list = list(resnet.children())[:-1]      # delete the last fc layer.\n",
        "    self.resnet_module = nn.Sequential(*module_list)\n",
        "    self.linear_layer = nn.Linear(resnet.fc.in_features, embedding_size)\n",
        "    self.batch_norm = nn.BatchNorm1d(embedding_size, momentum=0.01)\n",
        "\n",
        "  # input_image로부터 feature 벡터 추출\n",
        "  def forward(self, input_images):\n",
        "    with torch.no_grad():\n",
        "      resnet_features = self.resnet_module(input_images)\n",
        "    resnet_features = resnet_features.reshape(resnet_features.size(0), -1)\n",
        "    final_features = self.batch_norm(self.linear_layer(resnet_features))\n",
        "    return final_features"
      ],
      "metadata": {
        "id": "G32hHK18xnwe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "  def __init__(self,embedding_size, hidden_layer_size, vocabulary_size, num_layers, max_seq_len=20):\n",
        "    \"\"\"\n",
        "    embedding_size : input_size()\n",
        "\n",
        "    hidden_layer_size : hidden_state의 feature 개수\n",
        "\n",
        "    vocabulary_size : 단어 사전에 들어 있는 단어(OR 토큰화된 단어 즉, 단어 인덱스)들의 개수\n",
        "\n",
        "    num_layer : LSTM 셀로 이뤄진 layer가 몇 개 있는지\n",
        "\n",
        "    max_seq_len : 문장 길이는 20단어가 최대. 그 이하면 빈 단어 토큰이 채워지고 그 이상이면 문장이 축소됨\n",
        "    \"\"\"\n",
        "    super(LSTMModel, self).__init__()\n",
        "    self.embedding_layer = nn.Embedding(vocabulary_size, embedding_size) # 단어 익덱스가 담긴 단어사전을 룩업 테이블 역할을 하는 임베딩에 입력하는 layer\n",
        "    self.lstm_layer = nn.LSTM(embedding_size, hidden_layer_size, num_layers, batch_first=True)\n",
        "    self.linear_layer = nn.Linear(hidden_layer_size, vocabulary_size) # output을 위한 layer\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "  def forward(self, input_features, caps, lens):\n",
        "    \"\"\"\n",
        "    lens : 입력시퀀스(문장, 여기서는 캡션)의 길이를 담은 리스트\n",
        "\n",
        "    image_feature(즉, input_feature) 벡터를 decode 하고 해당 이미지의 caption을 만들기(예측하기)\n",
        "\n",
        "    두번째 embeddings : input_features.unsqueeze(1)를 통해 input_feature의 두 번째 차원에 새로운 축을 추가하여 이미지의 특징 벡터가 (batch_size,1,feature_size)\n",
        "                        가 될 수 있게 함. 왜 이렇게 함?\n",
        "                        -> 캡션의 각 단어의 shape이 (batch_size, caption_length, embedding_size)인데 input_feature의 경우 (batch_size, feature_size)라 차원이\n",
        "                           안 맞아서.\n",
        "    \"\"\"\n",
        "    # 단어를 고정 길이의 실수 벡터로 변환한 것.\n",
        "    embeddings = self.embedding_layer(caps) # caps에 있는 단어 인덱스들을 임베딩 벡터로 변환\n",
        "    embeddings = torch.cat((input_features.unsqueeze(1), embeddings), 1)\n",
        "    lstm_input = pack_padded_sequence(embeddings, lens, batch_first=True) # padded된 시퀀스를 더 효율적으로 처리할 수 있도록 도와주는 메서드라고 함\n",
        "    # 모든 타입스텝, 모든 layer에서의 hidden_state들을 전부 반환\n",
        "    hidden_variables, _ = self.lstm_layer(lstm_input) # 원래 이 객체는 output, (hn, cn)을 반환\n",
        "    # lstm_layer로부터 최종적으로 얻은 output을 바탕으로 우리가 만든 LSTM모델의 최종적인 결과값 도출(결과값은 문장을 구성하는 단어들이 어떤 단어에 해당하는지의 확률)\n",
        "    model_outputs = self.linear_layer(hidden_variables[0]) # LSTM구조상 가장 처음이 가장 최신(혹은 최종)의 값인 t시점의 값이므로 그걸 linear_layer에 입력으로 넣기\n",
        "    return model_outputs\n",
        "\n",
        "    # 주어진 이미지에 대한 문장(캡션) 생성을 위한 메서드\n",
        "    def sample(self, input_features, lstm_states=None):\n",
        "      sampled_indicies = []\n",
        "      lstm_inputs = input_features.unsqueeze(1)\n",
        "      for i in range(self.max_seq_len):\n",
        "        hidden_variables, lstm_states = self.lstm_layer(lstm_inputs, lstm_states) # hiddens: (batch_size, 1, hidden_size)\n",
        "        # 각 배치에 대해 단어 사전의 모든 단어에 대한 확률을 담은 텐서를 반환\n",
        "        model_outputs = self.linear_layer(hidden_variables.squeeze(1)) # outputs:  (batch_size, vocab_size)\n",
        "        # 각각의 배치에서 확률이 높은 단어의 인덱스를 뽑아오기\n",
        "        _, predicted_outputs = model_outputs.max(1) # predicted: (batch_size)\n",
        "        sampled_indices.append(predicted_outputs)\n",
        "        lstm_inputs = self.embedding_layer(predicted_outputs) # inputs: (batch_size, embed_size)\n",
        "        lstm_inputs = lstm_inputs.unsqueeze(1) # inputs: (batch_size, 1, embed_size)\n",
        "        sampled_indices = torch.stack(sampled_indices, 1) # sampled_ids: (batch_size, max_seq_length)\n",
        "        return sampled_indices"
      ],
      "metadata": {
        "id": "nCmLfZp7_O-1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coco_json = COCO(\"/content/data_dir/annotations/captions_train2014.json\")\n",
        "coco_json.anns"
      ],
      "metadata": {
        "id": "dpuT7TCLG2Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제는 training loop를 실현할 차례. 학습이 말도 안 되게 오래걸림. 몇 달은 걸릴 거 같음. 그냥 책에 나온 결과를 보는 것으로 만족해야 함."
      ],
      "metadata": {
        "id": "SgbtLhZEIpK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 모델이 저장될 디렉토리 생성\n",
        "# if not os.path.exists('model_dir/'):\n",
        "#   os.makedirs('models_dir/')\n",
        "# 사전학습된 resnet을 위한 이미지 전처리, 정규화\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                         (0.229, 0.224, 0.225))])\n",
        "# 단어 사전 load\n",
        "with open('data_dir/vocabulary.pkl', 'rb') as f:\n",
        "  vocabulary = pickle.load(f)\n",
        "# data_loader생성\n",
        "custom_data_loader = get_loader('data_dir/resized_images', 'data_dir/annotations/captions_train2014.json', vocabulary, transform, 128, shuffle=True, num_workers=0)\n",
        "# 모델 생성\n",
        "encoder_model = CNNModel(256).to(device)\n",
        "decoder_model = LSTMModel(256, 512, len(vocabulary), 1).to(device)\n",
        "# 손실함수와 최적화함수\n",
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "parameters = list(decoder_model.parameters()) + list(encoder_model.linear_layer.parameters()) + list(encoder_model.batch_norm.parameters())\n",
        "optimizer = torch.optim.Adam(parameters, lr=0.001)\n",
        "# Train the models\n",
        "total_num_steps = len(custom_data_loader)\n",
        "for epoch in range(5):\n",
        "    for i, (imgs, caps, lens) in enumerate(custom_data_loader):\n",
        "        # Set mini-batch dataset\n",
        "        imgs = imgs.to(device)\n",
        "        caps = caps.to(device)\n",
        "        tgts = pack_padded_sequence(caps, lens, batch_first=True)[0]\n",
        "        # Forward, backward and optimize\n",
        "        feats = encoder_model(imgs)\n",
        "        outputs = decoder_model(feats, caps, lens)\n",
        "        loss = loss_criterion(outputs, tgts)\n",
        "        decoder_model.zero_grad()\n",
        "        encoder_model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # 훈련 로그 찍기\n",
        "        if i % 10 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
        "                  .format(epoch, 5, i, total_num_steps, loss.item(),\n",
        "                          np.exp(loss.item())))\n",
        "        # 체크포인트 저장\n",
        "        if (i+1) % 1000 == 0:\n",
        "            torch.save(decoder_model.state_dict(), os.path.join('models_dir/', 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
        "            torch.save(encoder_model.state_dict(), os.path.join('models_dir/', 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))"
      ],
      "metadata": {
        "id": "4EVL9ByyIvem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이건 prediction. 모델 훈련을 못 시켰으니 얘도 돌리진x"
      ],
      "metadata": {
        "id": "pxllSI8bszNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_file_path = 'sample.jpg'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def load_image(image_file_path, transform=None):\n",
        "    img = Image.open(image_file_path).convert('RGB')\n",
        "    img = img.resize([224, 224], Image.LANCZOS)\n",
        "    if transform is not None:\n",
        "        img = transform(img).unsqueeze(0)\n",
        "    return img\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                         (0.229, 0.224, 0.225))])\n",
        "\n",
        "\n",
        "with open('data_dir/vocabulary.pkl', 'rb') as f:\n",
        "    vocabulary = pickle.load(f)\n",
        "\n",
        "\n",
        "encoder_model = CNNModel(256).eval()\n",
        "decoder_model = LSTMModel(256, 512, len(vocabulary), 1)\n",
        "encoder_model = encoder_model.to(device)\n",
        "decoder_model = decoder_model.to(device)\n",
        "# 학습된 모델 가져오기\n",
        "encoder_model.load_state_dict(torch.load('models_dir/encoder-2-3000.ckpt'))\n",
        "decoder_model.load_state_dict(torch.load('models_dir/decoder-2-3000.ckpt'))\n",
        "img = load_image(image_file_path, transform)\n",
        "img_tensor = img.to(device)\n",
        "# 이미지로부터 캡션 생성\n",
        "feat = encoder_model(img_tensor)\n",
        "sampled_indices = decoder_model.sample(feat)\n",
        "sampled_indices = sampled_indices[0].cpu().numpy() # (1, max_seq_length) -> (max_seq_length)\n",
        "predicted_caption = []\n",
        "for token_index in sampled_indices:\n",
        "    word = vocabulary.i2w[token_index]\n",
        "    predicted_caption.append(word)\n",
        "    if word == '<end>':\n",
        "        break\n",
        "predicted_sentence = ' '.join(predicted_caption)\n",
        "# 결과 출력\n",
        "%matplotlib inline\n",
        "print (predicted_sentence)\n",
        "img = Image.open(image_file_path)\n",
        "plt.imshow(np.asarray(img))"
      ],
      "metadata": {
        "id": "8t4TGlmOb2cN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}